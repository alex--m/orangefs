/* 
 * (C) 2001 Clemson University and The University of Chicago 
 *
 * See COPYING in top-level directory.
 */

/*
 *  PVFS2 server state machine for driving I/O operations (read and write).
 */

#include <string.h>
#include <assert.h>

#include "server-config.h"
#include "pvfs2-server.h"
#include "pvfs2-attr.h"
#include "pvfs2-request.h"
#include "pint-distribution.h"
#include "pint-request.h"
#include "pvfs2-internal.h"
#include "pint-cached-config.h"



enum
{
    NONE_CONTACTED_SUCCESSFULLY = 100,
    FLOWS_COMPLETED             = 200,
    PRIMARY_FLOW                = 300,
    NEXT_FLOW                   = 400,
    NOT_IN_USE                  = 500
};


static int contact_replicate_comp_fn(void *v_p, struct PVFS_server_resp *resp_p, int i);

%%

machine pvfs2_replicate_prime_sm
{
    state prelude
    {
        jump pvfs2_prelude_sm;
        success => contact_replicate_servers;
        default => send_initial_ack_to_client;
    }

    state contact_replicate_servers
    {
        jump pvfs2_contact_replicate_servers_sm;
        success => send_initial_ack_to_client;
        default => send_nak_to_client;
    }

    state send_nak_to_client
    {
        run io_send_nak_to_client;
        default => release;
    }

    state send_initial_ack_to_client
    {
        /* as long as recoverable errors occurred in pvfs2_contact_replicate_servers_sm, 
         * then we will always send a positive ack back to the client, because we will 
         * always attempt to write to the local server, which hasn't been attempted at 
         * this point in the code.  we may change this response later, once we know how 
         * we are going to handle replicate servers that are unreachable.
         */
        run io_send_ack;
        success => start_flow;
        default => release;
    }

    state start_flow
    {
        run io_start_flow;
        default => check_flow_completion;
    }

    state check_flow_completion
    {
        run replicate_check_flow_completion;
        FLOWS_COMPLETED => send_completion_ack_to_client;
        default => check_flow_completion;
    }

    state send_completion_ack_to_client
    {
        run io_send_completion_ack;
        default => release;
    }

    state release
    {
        run io_release;
        default => cleanup;
    }

    state cleanup
    {
        run io_cleanup;
        default => terminate;
    }
}/*end machine pvfs2_replicate_sm*/


nested machine pvfs2_contact_replicate_servers_sm
{
    state setup_msgpairs
    {
        run contact_setup_msgpairs;
        default => xfer_msgpairs;
    }

    state xfer_msgpairs
    {
        jump pvfs2_msgpairarray_sm;
        default => contact_post_bmi_recv_for_final_ack;
    }

    state contact_post_bmi_recv_for_final_ack
    {
        run contact_post_bmi_recv_for_final_ack;
        default => return;
    }

}/*end machine pvfs2_contact_replicate_servers_sm*/


%%

/*
 * Function: io_send_nak_to_client()
 *
 * Params:   server_op *s_op, 
 *           job_status_s* js_p
 *
 * Pre:      error code has been set in job status for us to
 *           report to client
 *
 * Post:     response has been sent to client
 *            
 * Returns:  int
 *
 * Synopsis: fills in a response to the I/O request, encodes it,
 *           and sends it to the client via BMI.  
 */
static int io_send_nak_to_client(struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int ret;
    job_id_t tmp_id;
    struct server_configuration_s *user_opts = get_server_config_struct();


    s_op->resp.status = js_p->error_code;

    ret = PINT_encode(&s_op->resp, PINT_ENCODE_RESP, &(s_op->encoded),
                      s_op->addr, s_op->decoded.enc_type);
    if (ret < 0)
    {
        gossip_lerr("Server: IO SM: PINT_encode() failure(%d). Could not send NAK to client\n",ret);
        js_p->error_code = ret;
        return SM_ACTION_COMPLETE;
    }

    ret = job_bmi_send_list(
        s_op->addr, s_op->encoded.buffer_list, s_op->encoded.size_list,
        s_op->encoded.list_count, s_op->encoded.total_size,
        s_op->tag, s_op->encoded.buffer_type, 0, smcb, 0, js_p,
        &tmp_id, server_job_context, user_opts->server_job_bmi_timeout,
        s_op->req->hints);

    return ret;
}/* end function io_send_nak_to_client */



/*
 * Function: io_send_ack()
 *
 * Params:   server_op *s_op, 
 *           job_status_s* js_p
 *
 * Pre:      error code has been set in job status for us to
 *           report to client
 *
 * Post:     response has been sent to client
 *            
 * Returns:  int
 *
 * Synopsis: fills in a response to the I/O request, encodes it,
 *           and sends it to the client via BMI.  Note that it may
 *           send either positive or negative acknowledgements.
 *           
 */
static int io_send_ack(
        struct PINT_smcb *smcb, job_status_s *js_p)
{
    gossip_debug(GOSSIP_MIRROR_DEBUG,"Executing io_send_ack (io.sm)....\n");
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int i, err = -PVFS_EIO;
    job_id_t tmp_id;
    struct server_configuration_s *user_opts = get_server_config_struct();
    PVFS_handle *handle = NULL;

    gossip_err("Replication Number of Copies (%d)\n",(int)s_op->req->u.io.replication_number_of_copies);
    for (i=0; i<s_op->req->u.io.replication_number_of_copies; i++)
        gossip_err("replication handle[%d]:(%llu)\n",i,llu(s_op->req->u.io.replication_handles[i]));   

    handle = (PVFS_handle *)PINT_hint_get_value_by_type(s_op->req->hints,PINT_HINT_HANDLE,NULL);

    gossip_err("Metadata handle(%llu)found in hints.\n",handle?llu(*handle):0);

    /* this is where we report the file size to the client before
     * starting the I/O transfer, or else report an error if we
     * failed to get the size, or failed for permission reasons
     */
    s_op->resp.status = js_p->error_code;
    if (!js_p->error_code)
    {
       s_op->resp.u.io.bstream_size = s_op->ds_attr.u.datafile.b_size;
    }

    gossip_debug(GOSSIP_MIRROR_DEBUG,"\tbstream_size:%d\n"
                                    ,(int)s_op->resp.u.io.bstream_size);

    err = PINT_encode(&s_op->resp, PINT_ENCODE_RESP, &(s_op->encoded),
                      s_op->addr, s_op->decoded.enc_type);
    if (err < 0)
    {
        gossip_lerr("Server: IO SM: PINT_encode() failure.\n");
        js_p->error_code = err;
        return SM_ACTION_COMPLETE;
    }

    err = job_bmi_send_list(
        s_op->addr, s_op->encoded.buffer_list, s_op->encoded.size_list,
        s_op->encoded.list_count, s_op->encoded.total_size,
        s_op->tag, s_op->encoded.buffer_type, 0, smcb, 0, js_p,
        &tmp_id, server_job_context, user_opts->server_job_bmi_timeout,
        s_op->req->hints);

    return err;
}

/*
 * Function: io_start_flow()
 *
 * Params:   server_op *s_op, 
 *           job_status_s* js_p
 *
 * Pre:      all of the previous steps have succeeded, so that we
 *           are ready to actually perform the I/O
 *
 * Post:     I/O has been carried out
 *            
 * Returns:  int
 *
 * Synopsis: this is the most important part of the state machine.
 *           we setup the flow descriptor and post it in order to 
 *           carry out the data transfer
 *           
 */
static PINT_sm_action io_start_flow(
        struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int i, err;
    job_id_t tmp_id;
    struct server_configuration_s *user_opts = get_server_config_struct();
    struct filesystem_configuration_s *fs_conf;

    js_p->error_code = 0;
    js_p->status_user_tag = NOT_IN_USE;
        
    s_op->u.io.flow_d = PINT_flow_alloc();
    if (!s_op->u.io.flow_d)
    {
        js_p->error_code = -PVFS_ENOMEM;
        return SM_ACTION_COMPLETE;
    }

    s_op->u.io.flow_d->hints = s_op->req->hints;

    s_op->u.io.flow_d->next_dest = calloc(s_op->req->u.io.replication_number_of_copies
                                         ,sizeof(*s_op->u.io.flow_d->next_dest));
    if (!s_op->u.io.flow_d->next_dest)
    {
        gossip_lerr("Error allocating memory for next_dest structures.\n");
        js_p->error_code = -PVFS_ENOMEM;
        return SM_ACTION_COMPLETE;
    }
    s_op->u.io.flow_d->next_dest_count = s_op->req->u.io.replication_number_of_copies;

    /* pass along replication status information through the flow descriptor */
    s_op->u.io.flow_d->repl_d                  = s_op->u.io.replicate_d;
    s_op->u.io.flow_d->repl_d_repl_count       = s_op->u.io.replicate_d_repl_count;
    s_op->u.io.flow_d->repl_d_total_count      = s_op->u.io.replicate_d_total_count;
    s_op->u.io.flow_d->repl_d_local_flow_index = s_op->u.io.replicate_d_local_flow_index;

    gossip_err("%s:s_op->u.io.flow_d->repl_d(%p) s_op->u.io.replicate_d(%p)\n"
               ,__func__,s_op->u.io.flow_d->repl_d,s_op->u.io.replicate_d);
    for (i=0; i<s_op->u.io.flow_d->repl_d_repl_count; i++)
    {
        gossip_err("&s_op->u.io.flow_d->repl_d[%d]:(%p) &s_op->u.io.replicate_d[%d]:(%p)\n"
       ,i,&s_op->u.io.flow_d->repl_d[i],i,&s_op->u.io.replicate_d[i]);
    }


    /* we still have the file size stored in the response structure 
     * that we sent in the previous state, other details come from
     * request
     */
    s_op->u.io.flow_d->file_data.fsize     = s_op->resp.u.io.bstream_size;
    s_op->u.io.flow_d->file_data.dist      = s_op->req->u.io.io_dist;
    s_op->u.io.flow_d->file_data.server_nr = s_op->req->u.io.server_nr;
    s_op->u.io.flow_d->file_data.server_ct = s_op->req->u.io.server_ct;

    /* on writes, we allow the bstream to be extended at EOF */
    if (s_op->req->u.io.io_type == PVFS_IO_WRITE)
    {
        gossip_debug(GOSSIP_IO_DEBUG, "io_start_flow() issuing flow to "
                     "write data.\n");
        s_op->u.io.flow_d->file_data.extend_flag = 1;
    }
    else
    {
        gossip_debug(GOSSIP_IO_DEBUG, "io_start_flow() issuing flow to "
                     "read data.\n");
        s_op->u.io.flow_d->file_data.extend_flag = 0;
    }

    gossip_err("file_req_offset:%lld\n",(long long int)s_op->req->u.io.file_req_offset);
    s_op->u.io.flow_d->file_req        = s_op->req->u.io.file_req;
    s_op->u.io.flow_d->file_req_offset = s_op->req->u.io.file_req_offset;
    s_op->u.io.flow_d->mem_req         = NULL;
    s_op->u.io.flow_d->aggregate_size  = s_op->req->u.io.aggregate_size;
    s_op->u.io.flow_d->tag             = s_op->tag;
    
    s_op->u.io.flow_d->user_ptr          = NULL;
    s_op->u.io.flow_d->type              = s_op->req->u.io.flow_type;
    s_op->u.io.flow_d->total_transferred = 0;

    fs_conf = PINT_config_find_fs_id(user_opts, 
                                     s_op->req->u.io.fs_id);
    if(fs_conf)
    {
        /* pick up any buffer settings overrides from fs conf */
        s_op->u.io.flow_d->buffer_size      = fs_conf->fp_buffer_size;
        s_op->u.io.flow_d->buffers_per_flow = fs_conf->fp_buffers_per_flow;
    }

    gossip_debug(GOSSIP_IO_DEBUG, "flow: fsize: %lld, " 
        "server_nr: %d, server_ct: %d\n",
        lld(s_op->u.io.flow_d->file_data.fsize),
        (int)s_op->u.io.flow_d->file_data.server_nr,
        (int)s_op->u.io.flow_d->file_data.server_ct);

    gossip_debug(GOSSIP_IO_DEBUG, "      file_req_offset: %lld, "
        "aggregate_size: %lld, handle: %llu\n", 
        lld(s_op->u.io.flow_d->file_req_offset),
        lld(s_op->u.io.flow_d->aggregate_size),
        llu(s_op->req->u.io.handle));

    /* set endpoints depending on type of io requested */
    if (s_op->req->u.io.io_type == PVFS_IO_WRITE)
    {
        s_op->u.io.flow_d->src.endpoint_id   = BMI_ENDPOINT;
        s_op->u.io.flow_d->src.u.bmi.address = s_op->addr;
        s_op->u.io.flow_d->dest.endpoint_id  = REPLICATION_ENDPOINT;

        /* it is possible that NONE of the replicate servers are RUNNING, depending on the results of the
         * initial contact.
         */
        for (i=0; i<s_op->u.io.flow_d->next_dest_count && s_op->u.io.replicate_d[i].endpt_status.state == RUNNING; i++)
        { 
            s_op->u.io.flow_d->next_dest[i].endpoint_id       = BMI_ENDPOINT;
            s_op->u.io.flow_d->next_dest[i].u.bmi.address     = s_op->u.io.replicate_d[i].svr_addr;
            s_op->u.io.flow_d->next_dest[i].u.bmi.tag         = s_op->u.io.replicate_d[i].session_tag;
        }

        s_op->u.io.flow_d->dest.u.trove.handle  = s_op->req->u.io.handle;
        s_op->u.io.flow_d->dest.u.trove.coll_id = s_op->req->u.io.fs_id;
    }
    else if (s_op->req->u.io.io_type == PVFS_IO_READ)
    {
        s_op->u.io.flow_d->src.endpoint_id = TROVE_ENDPOINT;
        s_op->u.io.flow_d->src.u.trove.handle = s_op->req->u.io.handle;
        s_op->u.io.flow_d->src.u.trove.coll_id = s_op->req->u.io.fs_id;
        s_op->u.io.flow_d->dest.endpoint_id = BMI_ENDPOINT;
        s_op->u.io.flow_d->dest.u.bmi.address = s_op->addr;
    }
    else
    {
        gossip_lerr("Server: IO SM: unknown IO type requested.\n");
        js_p->error_code = -PVFS_EINVAL;
        return SM_ACTION_COMPLETE;
    }

    gossip_err("%s\n",__func__);
    for (i=0; i<s_op->u.io.flow_d->repl_d_total_count; i++)
    {
       replication_endpoint_status_print(&(s_op->u.io.flow_d->repl_d[i].endpt_status),1);
    }

    gossip_debug(GOSSIP_IO_DEBUG,"\tabout to issue job_flow...\n");
    err = job_flow(s_op->u.io.flow_d, smcb, PRIMARY_FLOW, js_p, &tmp_id
                  ,server_job_context
                  ,user_opts->server_job_flow_timeout * (s_op->u.io.replicate_d_total_count)
                  ,s_op->req->hints);

    gossip_debug(GOSSIP_IO_DEBUG,"\treturn code from job_flow "
                                 "submission:%d\n"
                                ,err);

    return err;
}

/*
 * Function: io_release()
 *
 * Params:   server_op *b, 
 *           job_status_s* js_p
 *
 * Pre:      we are done with all steps necessary to service
 *           request
 *
 * Post:     operation has been released from the scheduler
 *
 * Returns:  int
 *
 * Synopsis: releases the operation from the scheduler
 */
static PINT_sm_action io_release(
        struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int ret = 0;
    job_id_t i;

    /*
      tell the scheduler that we are done with this operation (if it
      was scheduled in the first place)
    */
    ret = job_req_sched_release(
        s_op->scheduled_id, smcb, 0, js_p, &i, server_job_context);
    return ret;
}

/*
 * Function: io_cleanup()
 *
 * Params:   server_op *b, 
 *           job_status_s* js_p
 *
 * Pre:      all jobs done, simply need to clean up
 *
 * Post:     everything is free
 *
 * Returns:  int
 *
 * Synopsis: free up any buffers associated with the operation,
 *           including any encoded or decoded protocol structures
 */
static PINT_sm_action io_cleanup(
        struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    char status_string[64] = {0};

    PVFS_strerror_r(s_op->resp.status, status_string, 64);
    PINT_ACCESS_DEBUG(s_op, GOSSIP_ACCESS_DEBUG, "finish (%s)\n", status_string);

    if (s_op->u.io.flow_d)
    {
        PINT_flow_free(s_op->u.io.flow_d);
    }


    /* let go of our encoded response buffer, if we appear to have
     * made one
     */
    if (s_op->encoded.total_size)
    {
        PINT_encode_release(&s_op->encoded, PINT_ENCODE_RESP);
    }

    return(server_state_machine_complete(smcb));
}

/*
 * Function: io_send_completion_ack()
 *
 * Params:   server_op *s_op, 
 *           job_status_s* js_p
 *
 * Pre:      flow is completed so that we can report its status
 *
 * Post:     if this is a write, response has been sent to client
 *           if this is a read, do nothing
 *            
 * Returns:  int
 *
 * Synopsis: fills in a response to the I/O request, encodes it,
 *           and sends it to the client via BMI.  Note that it may
 *           send either positive or negative acknowledgements.
 *           
 */
static PINT_sm_action io_send_completion_ack(
        struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int i, err = -PVFS_EIO;
    job_id_t tmp_id;
    struct server_configuration_s *user_opts = get_server_config_struct();
    
    gossip_debug(GOSSIP_IO_DEBUG,"Executing io_send_completion_ack.\n");
    

    gossip_err("js_p->error_code:(%d)\n",(int)js_p->error_code);

    /* we only send this trailing ack if we are working on a write
     * operation; otherwise just cut out early
     */
    if (s_op->req->u.io.io_type == PVFS_IO_READ)
    {
        js_p->error_code = 0;
        return SM_ACTION_COMPLETE;
    }

    /* release encoding of the first ack that we sent */
    PINT_encode_release(&s_op->encoded, PINT_ENCODE_RESP);

    /* zero size for safety */
    s_op->encoded.total_size = 0;

    /*
      fill in response 
    */
    s_op->resp.op = PVFS_SERV_REPL_WRITE_COMPLETION;  

    /* determine the status based on the primary flow with consideration
     * from the "next" flows.
     */

    gossip_err("Primary Error Code:(%d)\n",s_op->u.io.primary_error_code);
    gossip_err("Total Transferred(%lld)\n",(long long int)s_op->u.io.flow_d->total_transferred);

    for (i=0; i<s_op->u.io.replicate_d_total_count; i++)
    {
        replication_endpoint_status_print(&(s_op->u.io.replicate_d[i].endpt_status),1);
        gossip_err("%s:error_code(%d) \tbytes written(%d).\n",__func__
                                                             ,(int)s_op->u.io.replicate_d[i].endpt_status.error_code
                                                             ,(int)s_op->u.io.replicate_d[i].endpt_status.writes_completed_bytes);
    }

    /* primary_error_code will be zero, unless there is an error within the state machine.  When the 
     * primary_error_code is zero, then the client is expected to process the data in the response
     * to determine the next course of action.
     */

    if (s_op->u.io.primary_error_code)
    {
       s_op->resp.status = s_op->u.io.primary_error_code;
    }
    else
    {
      /* if we send a non-zero response status, then the decoding mechanism does not
       * decode the rest of the response.  So, if we want the client to process the
       * data, we MUST set resp.status to zero.
       */ 
      s_op->resp.status = 0;
      s_op->resp.u.repl_write_completion.endpt_status_count = s_op->u.io.replicate_d_total_count;
      s_op->resp.u.repl_write_completion.endpt_status = calloc(s_op->u.io.replicate_d_total_count,
                                                               sizeof(*s_op->resp.u.repl_write_completion.endpt_status));
      if (!s_op->resp.u.repl_write_completion.endpt_status)
      {
          s_op->resp.status = -PVFS_ENOMEM;
      }
      else
      {
          for (i=0; i<s_op->u.io.replicate_d_total_count; i++)
          {
             s_op->resp.u.repl_write_completion.endpt_status[i] = s_op->u.io.replicate_d[i].endpt_status;
          }
      }
    }

    err = PINT_encode(
        &s_op->resp, PINT_ENCODE_RESP, &(s_op->encoded),
        s_op->addr, s_op->decoded.enc_type);

    if (err < 0)
    {
        gossip_lerr("Server: IO SM: PINT_encode() failure.\n");
        js_p->error_code = err;
        return SM_ACTION_COMPLETE;
    }

    gossip_debug(GOSSIP_IO_DEBUG,"\ts_op->tag:%d\n",s_op->tag);

    err = job_bmi_send_list(
        s_op->addr, s_op->encoded.buffer_list, s_op->encoded.size_list,
        s_op->encoded.list_count, s_op->encoded.total_size, s_op->tag,
        s_op->encoded.buffer_type, 0, smcb, 0, js_p, &tmp_id,
        server_job_context, user_opts->server_job_bmi_timeout,
        s_op->req->hints);

    gossip_debug(GOSSIP_IO_DEBUG,"return code from sending ack:%d\n"
                                ,err);

    return err;
}

static PINT_sm_action contact_setup_msgpairs(struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op                 = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    struct PVFS_server_req *replicate_prime_req = s_op->req;
    struct PVFS_servreq_io *io                  = &replicate_prime_req->u.io;
    PINT_sm_msgarray_op *msgarray_op            = &s_op->msgarray_op;
    PINT_sm_msgpair_state *msg_p;
    int ret,i;
 
//RAL TODO: What if one of the replicate servers is local? I am told that we won't allow duplicate data on a server.

    js_p->error_code = 0;

    /* number of replicas in the replicate_d array*/
    s_op->u.io.replicate_d_repl_count       = io->replication_number_of_copies;

    /* size of the replicate_d array */
    s_op->u.io.replicate_d_total_count      = io->replication_number_of_copies + 1;

    /* absolute index into the replicate_d array that represents the local flow data */
    s_op->u.io.replicate_d_local_flow_index = io->replication_number_of_copies;

    /* create storage for the status array */
    s_op->u.io.replicate_d = calloc(s_op->u.io.replicate_d_total_count,
                                    sizeof(*s_op->u.io.replicate_d));
    if (!s_op->u.io.replicate_d)
    {
       gossip_lerr("Error allocating memory for replicate_d.\n");
       js_p->error_code = -PVFS_ENOMEM;
       goto error_exit;
    }
    
    /* set state to PENDING for all replicas */
    for (i=0; i<s_op->u.io.replicate_d_repl_count; i++)
    {
        s_op->u.io.replicate_d[i].endpt_status.state = PENDING;
    }

    /* set state to RUNNING for the local flow */
    s_op->u.io.replicate_d[s_op->u.io.replicate_d_local_flow_index].endpt_status.state    = RUNNING;
    s_op->u.io.replicate_d[s_op->u.io.replicate_d_local_flow_index].endpt_status.handle   = io->handle;

    /* initialize msgpairarray parameters */
    PINT_serv_init_msgarray_params( s_op, s_op->target_fs_id );
    
    /* initialize msgpair structures, one for each replica server */
    ret = PINT_msgpairarray_init( msgarray_op, s_op->u.io.replicate_d_repl_count);    
    if (ret)
    {
       gossip_lerr("Error initializing msgpairarray.\n");
       js_p->error_code = ret;
       goto error_exit;
    }

    /* setup PVFS_SERV_REPLICATE_NEXT requests, one for each replica (msgarray[0] maps to replication_handles[0]). */
    for (i=0; i<msgarray_op->count; i++)
    {
        msg_p = &msgarray_op->msgarray[i];

        msg_p->fs_id      = s_op->target_fs_id;
        msg_p->handle     = io->replication_handles[i];
        msg_p->retry_flag = PVFS_MSGPAIR_RETRY;
        msg_p->comp_fn    = contact_replicate_comp_fn;
        
        ret=PINT_cached_config_map_to_server( &msg_p->svr_addr
                                             ,msg_p->handle
                                             ,s_op->target_fs_id);
        if (ret)
        {
           gossip_lerr("Error mapping handle(%llu) to BMI server address.\n",llu(msg_p->handle));
           js_p->error_code = ret;
           goto error_exit;
        }

        /* pass along the data from PVFS_SERV_REPLICATE_PRIME request*/
        PINT_SERVREQ_REPLICATE_NEXT_FILL(*replicate_prime_req
                                         ,msg_p->req
                                         ,io->replication_handles[i]);
        gossip_err("%s(%d):msg_p->req.u.io.flow_type(%d).\n",__func__,i,(int)msg_p->req.u.io.flow_type);

        /* save handle for each replica for later use */
        s_op->u.io.replicate_d[i].endpt_status.handle   = msg_p->handle;

    }/*end for*/ 

    PINT_sm_push_frame(smcb,0,msgarray_op);

    return SM_ACTION_COMPLETE;

error_exit:
    if (s_op->u.io.replicate_d)
    {
        free(s_op->u.io.replicate_d);
    }

    PINT_msgpairarray_destroy(msgarray_op);

    s_op->u.io.replicate_d = NULL;
    s_op->u.io.replicate_d_repl_count = 0;
    s_op->u.io.replicate_d_local_flow_index = 0;

    return SM_ACTION_COMPLETE;
}/*end contact_setup_msgpairs*/


static int contact_replicate_comp_fn(void *v_p, struct PVFS_server_resp *resp_p, int i)
{
    PINT_smcb *smcb = v_p;
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_MSGPAIR_PARENT_SM);
    PINT_sm_msgpair_state *msg_p = &s_op->msgarray_op.msgarray[i];

    /* Here, we will capture the response and session tag from the PVFS_SERV_REPLICATE_NEXT 
     * request.  The response pertains only to the inital ACK and start of the flow on
     * the replicate server.  We will retain the status from each response and then
     * check it when we return from msgpairarray.
     */

    /* store status and session tag. */

    gossip_err("Response %d of %d.\n",i,s_op->msgarray_op.count);
    gossip_err("Bstream size(%d) for handle(%llu)\n",(int)resp_p->u.io.bstream_size
                                                     ,llu(msg_p->handle));
    gossip_err("Response status (%d).\n",resp_p->status);
    gossip_err("Session tag (%d).\n",msg_p->session_tag);    
    
    s_op->u.io.replicate_d[i].endpt_status.error_code  = resp_p->status;
    s_op->u.io.replicate_d[i].session_tag              = msg_p->session_tag;
    s_op->u.io.replicate_d[i].svr_addr                 = msg_p->svr_addr;
    s_op->u.io.replicate_d[i].handle                   = msg_p->handle;

    if (s_op->u.io.replicate_d[i].endpt_status.error_code)
    {
        s_op->u.io.replicate_d[i].endpt_status.state = FAILED_INITIAL_CONTACT;
    }
    else
    {
        s_op->u.io.replicate_d[i].endpt_status.state = RUNNING;
    }

    resp_p->status=0;

    return (0);
}/*end contact_replicate_comp_fn*/

static PINT_sm_action contact_post_bmi_recv_for_final_ack(struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    int max_resp_sz=0;
    struct server_configuration_s *server_opts = get_server_config_struct();
    struct filesystem_configuration_s *fs_config;
    int i, ret, error_index, j;
    job_status_s replicate_recv_status = {0};
    job_id_t *replicate_recv_job_id = NULL;


    /* clean up msgpairarray from the previous setp */
    PINT_msgpairarray_destroy(&(s_op->msgarray_op));

    /* allocate space for the bmi recv job id's */
    replicate_recv_job_id = calloc(s_op->u.io.replicate_d_repl_count,sizeof(*replicate_recv_job_id));
    if ( !replicate_recv_job_id )
    {
       /* can't continue if server is having memory issues */
       js_p->error_code = -PVFS_ENOMEM;
       return SM_ACTION_COMPLETE;
    }


    /* get encoding info for this filesystem */
    fs_config = PINT_config_find_fs_id(server_opts,s_op->target_fs_id);

    /* calculate how much space is needed to receive the final ack from the replicate servers */
    max_resp_sz = PINT_encode_calc_max_size( PINT_ENCODE_RESP
                                            ,PVFS_SERV_WRITE_COMPLETION
                                            ,fs_config->encoding );

    /* for each server that was contacted successfully, post a BMI RECV, which waits for the final
     * acknowledgement (for writes only) that tells us the status of the replica flow.
     */
    for (i=0; i<s_op->u.io.replicate_d_repl_count && s_op->u.io.replicate_d[i].endpt_status.state  == RUNNING; i++)
    {
        s_op->u.io.replicate_d[i].encoded_resp_p = BMI_memalloc( s_op->u.io.replicate_d[i].svr_addr
                                                                ,max_resp_sz
                                                                ,BMI_RECV );
        if (!s_op->u.io.replicate_d[i].encoded_resp_p)
        {
            gossip_lerr("Error allocating memory for encoded response.\n");
            js_p->error_code = -PVFS_ENOMEM;
            error_index = i;
            goto error_exit;
        }
        ret = job_bmi_recv(s_op->u.io.replicate_d[i].svr_addr,
                           s_op->u.io.replicate_d[i].encoded_resp_p,
                           max_resp_sz,
                           s_op->u.io.replicate_d[i].session_tag,
                           BMI_PRE_ALLOC,
                           smcb,
                           NEXT_FLOW + i, /*the status_user_tag contains the index into the replicate_d array */
                           &replicate_recv_status,
                           &(replicate_recv_job_id[i]),
                           server_job_context,
                           server_opts->server_job_flow_timeout * s_op->req->u.io.replication_number_of_copies,
                           s_op->req->hints);
        gossip_err("%s:job_bmi_recv:ret(%d) \trecv_status[%d].error_code:(%d).\n"
                   ,__func__,ret,i,replicate_recv_status.error_code);
        if (ret == 1 && replicate_recv_status.error_code)
        {
           /* error posting job */

           /* Replicate-next requests will be cancelled by the job timer on their respective servers. Currently, */
           /* we have no way of cancelling a job on another server.                                              */
           s_op->u.io.replicate_d[i].endpt_status.error_code = replicate_recv_status.error_code;
           s_op->u.io.replicate_d[i].endpt_status.state      = FAILED_INITIAL_BMI_RECV_POST;
           ret=BMI_memfree(s_op->u.io.replicate_d[i].svr_addr
                          ,s_op->u.io.replicate_d[i].encoded_resp_p
                          ,max_resp_sz
                          ,BMI_RECV);
           ret=job_bmi_cancel(replicate_recv_job_id[i],server_job_context);
           continue;
        }
        else if (ret == 1)
        {
           /* immediate completion.  since we haven't started the flows yet, something is 
            * wrong with this picture! Replicate-next requests will cancel when their jobs timeout.
            */
           s_op->u.io.replicate_d[i].endpt_status.error_code = -PVFS_EINVAL;
           s_op->u.io.replicate_d[i].endpt_status.state = FAILED_INITIAL_BMI_RECV_POST;
           ret=BMI_memfree(s_op->u.io.replicate_d[i].svr_addr
                          ,s_op->u.io.replicate_d[i].encoded_resp_p
                          ,max_resp_sz
                          ,BMI_RECV);
           ret=job_bmi_cancel(replicate_recv_job_id[i],server_job_context);
           continue;
        }
        else if (ret != 0)
        {
           /* error adding job to job mgr. Need to cancel replicate-next request? */
           s_op->u.io.replicate_d[i].endpt_status.error_code = ret;
           s_op->u.io.replicate_d[i].endpt_status.state = FAILED_INITIAL_BMI_RECV_POST;
           ret=BMI_memfree(s_op->u.io.replicate_d[i].svr_addr
                          ,s_op->u.io.replicate_d[i].encoded_resp_p
                          ,max_resp_sz
                          ,BMI_RECV);
           ret=job_bmi_cancel(replicate_recv_job_id[i],server_job_context);
           continue;
        }
    }/*end for*/

    js_p->error_code = 0;
    
    return SM_ACTION_COMPLETE;


error_exit:
    /* we only hit this exit when we are unable to allocate memory. For such a severe error, we 
     * must shutdown everything for this request.
     */
    for (j=0; j<error_index; j++)
    {
       if (s_op->u.io.replicate_d[j].endpt_status.state != RUNNING)
       {
          /* this replicate prime request has already been shutdown */
          continue;
       }
       ret=BMI_memfree(s_op->u.io.replicate_d[j].svr_addr
                      ,s_op->u.io.replicate_d[j].encoded_resp_p
                      ,max_resp_sz
                      ,BMI_RECV);
       ret=job_bmi_cancel(replicate_recv_job_id[j],server_job_context);
    }    

    return SM_ACTION_COMPLETE;
}/*end contact_post_bmi_recv_for_final_ack*/




static PINT_sm_action replicate_check_flow_completion(struct PINT_smcb *smcb, job_status_s *js_p)
{
    struct PINT_server_op *s_op = PINT_sm_frame(smcb, PINT_FRAME_CURRENT);
    struct PINT_decoded_msg decoded_resp;
    struct PVFS_server_resp *resp;
    int index, ret, i;


    gossip_err("js_p->error_code(%d):js_p->status_user_tag(%d):js_p->actual_size(%d)\n",(int)js_p->error_code
                                                                                        ,(int)js_p->status_user_tag
                                                                                        ,(int)js_p->actual_size);

    gossip_err("Replicate-Prime.sm:%s:\n",__func__);
    for (i=0; i<s_op->u.io.replicate_d_total_count; i++)
    {
       replication_endpoint_status_print(&(s_op->u.io.replicate_d[i].endpt_status),1);
       gossip_err("%s:error_code(%d):(%p)) \twrites_completed_bytes(%d):(%p).\n"
                 ,__func__,(int)s_op->u.io.replicate_d[i].endpt_status.error_code
                          ,&(s_op->u.io.replicate_d[i].endpt_status.error_code)
                          ,(int)s_op->u.io.replicate_d[i].endpt_status.writes_completed_bytes
                          ,&(s_op->u.io.replicate_d[i].endpt_status.writes_completed_bytes));
    }


    /* Did we encounter an error before flows could be started? */
    if (js_p->status_user_tag == NOT_IN_USE)
    {
       if (js_p->error_code)
       {
          gossip_lerr("Error occurred before flows could be started(%d).\n",js_p->error_code);
          s_op->u.io.primary_error_code = js_p->error_code;
          js_p->error_code = FLOWS_COMPLETED;
          return SM_ACTION_COMPLETE;
       }
    }


    /* Which job just finished? Local? */
    if (js_p->status_user_tag == PRIMARY_FLOW)
    {
       gossip_err("Primary flow has completed..Actual amount written to local server's storage(%lld)\n"
                  ,(long long int)js_p->actual_size);
       for (index=0; index<s_op->u.io.replicate_d_repl_count && s_op->u.io.replicate_d[index].endpt_status.state >= RUNNING; index++)
       {
           if ( !s_op->u.io.replicate_d[index].received )
           {
              gossip_err("Primary flow completed BEFORE replicate flow (%llu) having state(%s).\n"
                         ,llu(s_op->u.io.replicate_d[index].handle)
                         ,get_replication_endpoint_state_as_string(s_op->u.io.replicate_d[index].endpt_status.state));
           }
       }
       s_op->u.io.primary_flow_received = 1;
       goto completion_exit;
    }

    /* replicate flow? */
    index = js_p->status_user_tag - NEXT_FLOW;
    s_op->u.io.replicate_d[index].received = 1;

    /* If replicate-next sm sent back a non-zero error code, then let that error override any
     * error that may have been discovered during the flow from replicate-prime.
     */
    if (js_p->error_code)
    {
       s_op->u.io.replicate_d[index].endpt_status.error_code = js_p->error_code;
       s_op->u.io.replicate_d[index].endpt_status.state = FAILED_TO_REPLICATE;
       goto completion_exit;
    }
    else if (s_op->u.io.replicate_d[index].endpt_status.state != RUNNING)
    {
       goto completion_exit;
    }

    /* if no errors from the internal flow(for this replica) nor replicate-next, then decode repsonse from "replica" server */
    ret = PINT_serv_decode_resp( s_op->target_fs_id
                                ,s_op->u.io.replicate_d[index].encoded_resp_p
                                ,&decoded_resp
                                ,&s_op->u.io.replicate_d[index].svr_addr
                                ,js_p->actual_size
                                ,&resp );
    if (ret != 0)
    {
       gossip_lerr("Error decoding resp from NEXT server for handle (%llu).\n"
                  ,llu(s_op->u.io.replicate_d[index].handle));
       PVFS_perror_gossip("replicate-prime decoding error",ret);
       s_op->u.io.replicate_d[index].endpt_status.error_code = ret;       
       s_op->u.io.replicate_d[index].endpt_status.state = FAILED_FINAL_DECODE;
       goto completion_exit;
    }

    /* set the number of bytes handled by this replica server */
    s_op->u.io.replicate_d[index].endpt_status.writes_completed_bytes = resp->u.write_completion.total_completed;

    gossip_err("Completion ACK returned from NEXT server for handle(%llu).\n"
               ,llu(s_op->u.io.replicate_d[index].handle));
    gossip_err("Amount of data written(%lld).\n",(long long int)resp->u.write_completion.total_completed);


    /* not sure if this is necessary; check on it */
    PINT_decode_release(&decoded_resp, PINT_ENCODE_RESP);


/* Are we done yet? */
completion_exit:

   /* check the local flow */
   if (!s_op->u.io.primary_flow_received)
   {
      js_p->error_code = 0;
      return SM_ACTION_DEFERRED;
   }

   /* check the replicate flows */
   for (index=0; index<s_op->u.io.replicate_d_repl_count && s_op->u.io.replicate_d[index].endpt_status.state >= RUNNING; index++)
   {
       if (!s_op->u.io.replicate_d[index].received)
       {
          js_p->error_code = 0;
          return SM_ACTION_DEFERRED;
       }
   }

   /* yep, we're done! */
   /* we will always send back a zero error code in the response to the client.  the client MUST look at the 
    * data in the repsonse to determine what should happen next on the client side.
    */
   s_op->u.io.primary_error_code = 0;

   js_p->error_code = FLOWS_COMPLETED;
   return SM_ACTION_COMPLETE;
}/*end replicate_check_completion*/




static int perm_io(PINT_server_op *s_op)
{
    int ret = -PVFS_EINVAL;
    int mask = 0;
    enum PVFS_io_type type = s_op->req->u.io.io_type;

    if (type == PVFS_IO_READ)
    {
        mask = PINT_CAP_READ;
    }
    else if (type == PVFS_IO_WRITE)
    {
        mask = PINT_CAP_WRITE;
    }

    if (s_op->req->capability.op_mask & mask)
    {
        ret = 0;
    }
    else
    {
        ret = -PVFS_EACCES;
    }

    return ret;
}

static enum PINT_server_req_access_type PINT_server_req_access_io(
    struct PVFS_server_req *req)
{
    if(req->u.io.io_type == PVFS_IO_READ)
    {
        return PINT_SERVER_REQ_READONLY;
    }
    return PINT_SERVER_REQ_MODIFY;
}

PINT_GET_OBJECT_REF_DEFINE(io);

struct PINT_server_req_params pvfs2_replicate_prime_params =
{
    .string_name = "replicate-prime",
    .perm = perm_io,
    .access_type = PINT_server_req_access_io,
    .sched_policy = PINT_SERVER_REQ_SCHEDULE,
    .get_object_ref = PINT_get_object_ref_io,
    .state_machine = &pvfs2_replicate_prime_sm
};

/*
 * Local variables:
 *  mode: c
 *  c-indent-level: 4
 *  c-basic-offset: 4
 * End:
 *
 * vim: ft=c ts=8 sts=4 sw=4 expandtab
 */
