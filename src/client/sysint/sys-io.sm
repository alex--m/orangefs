/* 
 * (C) 2003 Clemson University and The University of Chicago 
 *
 * See COPYING in top-level directory.
 */
#include <string.h>
#include <assert.h>

#include "client-state-machine.h"
#include "state-machine-fns.h"
#include "shared-state-methods.h"
#include "pvfs2-types-debug.h"
#include "pvfs2-debug.h"
#include "job.h"
#include "gossip.h"
#include "str-utils.h"
#include "pint-servreq.h"
#include "pint-cached-config.h"
#include "PINT-reqproto-encode.h"
#include "pint-util.h"

extern job_context_id pint_client_sm_context;

enum
{
    IO_NO_DATA = 132,
    IO_DATAFILE_TRANSFERS_COMPLETE,
    IO_RETRY,
    IO_GET_DATAFILE_SIZE
};

static int io_init(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_object_getattr_failure(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_datafile_setup_msgpairs(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_datafile_post_msgpairs(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_datafile_complete_operations(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_analyze_results(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_datafile_size_setup_msgpairs(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_datafile_size_failure(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_analyze_size_results(
    PINT_client_sm *sm_p, job_status_s *js_p);
static int io_cleanup(
    PINT_client_sm *sm_p, job_status_s *js_p);

/* misc helper functions */
static inline int complete_context_send_or_recv(
    PINT_client_sm *sm_p, job_status_s *js_p);
static inline int process_context_recv(
    PINT_client_io_ctx *cur_ctx,
    struct PINT_decoded_msg *decoded_resp,
    struct PVFS_server_resp **resp);
static inline int build_context_flow(
    PINT_client_sm *sm_p, PINT_client_io_ctx *cur_ctx,
    PVFS_object_attr *attr, struct PVFS_server_resp *resp);
static inline int process_context_recv_and_post_flow(
    PINT_client_sm *sm_p, job_status_s *js_p);
static inline int check_context_status(
    PINT_client_io_ctx *cur_ctx, int io_type,
    PVFS_size *total_size);
static inline int check_for_zero_fill_on_read(
    PINT_client_sm *sm_p);
static int io_find_target_datafiles(
    PVFS_Request mem_req, PVFS_Request file_req,
    PVFS_offset file_req_offset, PINT_dist *dist_p,
    PVFS_handle *input_handle_array, int input_handle_count,
    int *handle_index_array, int *handle_index_out_count);
static int io_datafile_size_comp_fn(
    void *v_p, struct PVFS_server_resp *resp_p, int index);

/* misc constants and helper macros */
#define IO_RECV_COMPLETED                                    1

/* possible I/O state machine phases (status_user_tag) */
#define IO_SM_PHASE_REQ_MSGPAIR_RECV                         0
#define IO_SM_PHASE_REQ_MSGPAIR_SEND                         1
#define IO_SM_PHASE_FLOW                                     2
#define IO_SM_PHASE_FINAL_ACK                                3
#define IO_SM_NUM_PHASES                                     4

#define STATUS_USER_TAG_TYPE(tag, type)                      \
((tag % IO_SM_NUM_PHASES) == type)
#define STATUS_USER_TAG_GET_INDEX(tag, type)                 \
(tag / IO_SM_NUM_PHASES)
#define STATUS_USER_TAG_IS_SEND_OR_RECV(tag)                 \
(STATUS_USER_TAG_TYPE(tag, IO_SM_PHASE_REQ_MSGPAIR_RECV) ||  \
 STATUS_USER_TAG_TYPE(tag, IO_SM_PHASE_REQ_MSGPAIR_SEND))

#define CLEAN_PRIVATE_MEMBERS(sm_p)                          \
do {                                                         \
    if (sm_p->u.io.datafile_index_array)                     \
    {                                                        \
        free(sm_p->u.io.datafile_index_array);               \
        sm_p->u.io.datafile_index_array = NULL;              \
    }                                                        \
    if (sm_p->msgarray &&                                    \
        (sm_p->msgarray != &sm_p->msgpair))                  \
    {                                                        \
        free(sm_p->msgarray);                                \
        sm_p->msgarray = NULL;                               \
        sm_p->msgarray_count = 0;                            \
    }                                                        \
    if (sm_p->u.io.contexts)                                 \
    {                                                        \
        free(sm_p->u.io.contexts);                           \
        sm_p->u.io.contexts = NULL;                          \
    }                                                        \
} while(0)

%%

machine pvfs2_client_io_sm(
    init,
    io_getattr_setup_msgpair,
    io_getattr_xfer_msgpair,
    io_getattr_failure,
    io_datafile_setup_msgpairs,
    io_datafile_post_msgpairs,
    io_datafile_complete_operations,
    io_analyze_results,
    io_datafile_size_setup_msgpairs,
    io_datafile_size_xfer_msgpairs,
    io_datafile_size_failure,
    io_analyze_size_results,
    io_cleanup)
{
    state init
    {
        run io_init;
        default => io_getattr_setup_msgpair;
    }

    state io_getattr_setup_msgpair
    {
        run PINT_sm_common_object_getattr_setup_msgpair;
        success => io_getattr_xfer_msgpair;
        default => io_getattr_failure;
    }

    state io_getattr_xfer_msgpair
    {
        jump pvfs2_client_getattr_acache_sm;
        success => io_datafile_setup_msgpairs;
        default => io_getattr_failure;
    }

    state io_getattr_failure
    {
        run io_object_getattr_failure;
        default => io_cleanup;
    }

    state io_datafile_setup_msgpairs
    {
        run io_datafile_setup_msgpairs;
        IO_NO_DATA => io_cleanup;
        success => io_datafile_post_msgpairs;
        default => io_cleanup;
    }

    state io_datafile_post_msgpairs
    {
        run io_datafile_post_msgpairs;
        default => io_datafile_complete_operations;
    }

    state io_datafile_complete_operations
    {
        run io_datafile_complete_operations;
        IO_DATAFILE_TRANSFERS_COMPLETE => io_analyze_results;
        default => io_datafile_complete_operations;
    }

    state io_analyze_results
    {
        run io_analyze_results;
        IO_RETRY => init;
        IO_GET_DATAFILE_SIZE => io_datafile_size_setup_msgpairs;
        default => io_cleanup;
    }

    state io_datafile_size_setup_msgpairs
    {
        run io_datafile_size_setup_msgpairs;
        success => io_datafile_size_xfer_msgpairs;
        default => io_datafile_size_failure;
    }

    state io_datafile_size_xfer_msgpairs
    {
        jump pvfs2_msgpairarray_sm;
        success => io_analyze_size_results;
        default => io_datafile_size_failure;
    }

    state io_datafile_size_failure
    {
        run io_datafile_size_failure;
        default => io_cleanup;
    }

    state io_analyze_size_results
    {
        run io_analyze_size_results;
        default => io_analyze_results;
    }

    state io_cleanup
    {
        run io_cleanup;
        default => terminate;
    }
}

%%

PVFS_error PVFS_isys_io(
    PVFS_object_ref ref,
    PVFS_Request file_req,
    PVFS_offset file_req_offset,
    void *buffer,
    PVFS_Request mem_req,
    PVFS_credentials *credentials,
    PVFS_sysresp_io *resp_p,
    enum PVFS_io_type io_type,
    PVFS_sys_op_id *op_id,
    void *user_ptr)
{
    PVFS_error ret = -PVFS_EINVAL;
    PINT_client_sm *sm_p = NULL;
    struct filesystem_configuration_s* cur_fs = NULL;
    struct server_configuration_s *server_config = NULL;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "PVFS_isys_io entered [%Lu]\n",
                 Lu(ref.handle));

    if ((ref.handle == PVFS_HANDLE_NULL) ||
        (ref.fs_id == PVFS_FS_ID_NULL) || (resp_p == NULL))
    {
        gossip_err("invalid (NULL) required argument\n");
        return ret;
    }

    if ((io_type != PVFS_IO_READ) && (io_type != PVFS_IO_WRITE))
    {
        gossip_err("invalid (unknown) I/O type specified\n");
        return ret;
    }

    server_config = PINT_get_server_config_struct(ref.fs_id);
    cur_fs = PINT_config_find_fs_id(server_config, ref.fs_id);
    PINT_put_server_config_struct(server_config);

    if (!cur_fs)
    {
        gossip_err("invalid (unknown) fs id specified\n");
        return ret;
    }

    /* look for zero byte operations */
    if ((PINT_REQUEST_TOTAL_BYTES(mem_req) == 0) ||
        (PINT_REQUEST_TOTAL_BYTES(file_req) == 0))
    {
        gossip_ldebug(GOSSIP_IO_DEBUG, "Warning: 0 byte I/O operation "
                      "attempted.\n");
        resp_p->total_completed = 0;
        return 0;
    }

    sm_p = (PINT_client_sm *)malloc(sizeof(*sm_p));
    if (sm_p == NULL)
    {
        return -PVFS_ENOMEM;
    }
    memset(sm_p, 0, sizeof(*sm_p));

    PINT_init_msgarray_params(&sm_p->msgarray_params);
    PINT_init_sysint_credentials(sm_p->cred_p, credentials);
    sm_p->u.io.io_type = io_type;
    sm_p->u.io.file_req = file_req;
    sm_p->u.io.file_req_offset = file_req_offset;
    sm_p->u.io.io_resp_p = resp_p;
    sm_p->u.io.mem_req = mem_req;
    sm_p->u.io.buffer = buffer; 
    sm_p->u.io.flowproto_type = cur_fs->flowproto;
    sm_p->u.io.encoding = cur_fs->encoding;
    sm_p->u.io.stored_error_code = 0;
    sm_p->u.io.retry_count = 0;
    sm_p->msgarray = NULL;
    sm_p->u.io.datafile_index_array = NULL;
    sm_p->u.io.datafile_count = 0;
    sm_p->u.io.total_size = 0;
    sm_p->u.io.size = 0;
    sm_p->u.io.size_array = NULL;
    sm_p->u.io.continue_analysis = 0;
    sm_p->u.io.saved_ret = 0;
    sm_p->u.io.saved_error_code = 0;
    sm_p->object_ref = ref;
    sm_p->ref_type = PVFS_TYPE_METAFILE;

    return PINT_client_state_machine_post(
        sm_p, PVFS_SYS_IO, op_id, user_ptr);
}

PVFS_error PVFS_sys_io(
    PVFS_object_ref ref,
    PVFS_Request file_req,
    PVFS_offset file_req_offset,
    void *buffer,
    PVFS_Request mem_req,
    PVFS_credentials *credentials,
    PVFS_sysresp_io *resp_p,
    enum PVFS_io_type io_type)
{
    PVFS_error ret = -PVFS_EINVAL, error = 0;
    PVFS_sys_op_id op_id;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "PVFS_sys_io entered\n");

    ret = PVFS_isys_io(ref, file_req, file_req_offset, buffer, mem_req,
                       credentials, resp_p, io_type, &op_id, NULL);
    if (ret)
    {
        PVFS_perror_gossip("PVFS_isys_io call", ret);
        error = ret;
    }
    else
    {
        ret = PINT_sys_wait(op_id, "io", &error);
        if (ret)
        {
            PVFS_perror_gossip("PVFS_sys_wait call", ret);
            error = ret;
        }
    }

    PINT_sys_release(op_id);
    return error;
}

/*******************************************************************/

static int io_init(PINT_client_sm *sm_p,
                   job_status_s *js_p)
{
    job_id_t tmp_id;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: io_init\n", sm_p);

    assert((js_p->error_code == 0) ||
           (js_p->error_code == IO_RETRY));

    if (js_p->error_code == IO_RETRY)
    {
        js_p->error_code = 0;

        CLEAN_PRIVATE_MEMBERS(sm_p);

        if (sm_p->op_cancelled)
        {
            js_p->error_code = -PVFS_ECANCEL;
            return 1;
        }

        return job_req_sched_post_timer(
            PVFS2_CLIENT_RETRY_DELAY, sm_p, 0, js_p, &tmp_id,
            pint_client_sm_context);
    }
    return 1;
}

static int io_object_getattr_failure(PINT_client_sm *sm_p,
                                     job_status_s *js_p)
{
    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: "
                 "io_object_getattr_failure\n", sm_p);

    if (sm_p->op_cancelled)
    {
        js_p->error_code = -PVFS_ECANCEL;
    }

    /*
      NOTE: this can happen if we're doing I/O on a file that was
      removed by another process
    */
    if (js_p->error_code == 0)
    {
        js_p->error_code = -PVFS_ENOENT;
    }

    sm_p->u.io.stored_error_code = js_p->error_code;
    return 1;
}

static int io_datafile_setup_msgpairs(PINT_client_sm *sm_p,
                                      job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, i = 0;
    PVFS_object_attr *attr = NULL;
    int target_datafile_count = 0;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: "
                 "io_datafile_setup_msgpairs\n", sm_p);

    if (sm_p->op_cancelled)
    {
        js_p->error_code = -PVFS_ECANCEL;
        return 1;
    }

    js_p->error_code = 0;

    attr = (sm_p->acache_hit ?
            &sm_p->pinode->attr :
            &sm_p->acache_attr);
    assert(attr);

    switch(attr->objtype)
    {
        case PVFS_TYPE_METAFILE:
            assert(attr->objtype == PVFS_TYPE_METAFILE);
            assert(attr->mask & PVFS_ATTR_META_DFILES);
            assert(attr->mask & PVFS_ATTR_META_DIST);
            assert(attr->u.meta.dist_size > 0);
            assert(attr->u.meta.dfile_array);
            assert(attr->u.meta.dfile_count > 0);
            break;
        case PVFS_TYPE_DIRECTORY:
            js_p->error_code = -PVFS_EISDIR;
            return 1;
        default:
            js_p->error_code = -PVFS_EBADF;
            return 1;
    }

    ret = PINT_dist_lookup(attr->u.meta.dist);
    if (ret)
    {
        PVFS_perror_gossip("PINT_dist_lookup failed; aborting I/O", ret);
        js_p->error_code = -PVFS_EBADF;
        return 1;
    }

    sm_p->u.io.datafile_index_array = (int *)malloc(
        (attr->u.meta.dfile_count * sizeof(int)));
    if (!sm_p->u.io.datafile_index_array)
    {
        goto malloc_error_exit;
    }
    memset(sm_p->u.io.datafile_index_array, 0,
           (attr->u.meta.dfile_count * sizeof(int)));

    ret = io_find_target_datafiles(
        sm_p->u.io.mem_req,
        sm_p->u.io.file_req,
        sm_p->u.io.file_req_offset,
        attr->u.meta.dist,
        attr->u.meta.dfile_array,
        attr->u.meta.dfile_count,
        sm_p->u.io.datafile_index_array,
        &target_datafile_count);

    assert(ret == 0);

    if (target_datafile_count == 0)
    {
        free(sm_p->u.io.datafile_index_array);
        sm_p->u.io.datafile_index_array = NULL;

        gossip_debug(GOSSIP_IO_DEBUG, "  datafile_setup_msgpairs: no "
                     "datafiles have data; aborting\n");

        js_p->error_code = IO_NO_DATA;
        return 1;
    }

    gossip_debug(GOSSIP_IO_DEBUG,
                 "  %s: %d datafiles "
                 "might have data\n", __func__, target_datafile_count);

    sm_p->u.io.contexts = (PINT_client_io_ctx *)malloc(
        (target_datafile_count * sizeof(PINT_client_io_ctx)));
    if (!sm_p->u.io.contexts)
    {
        goto malloc_error_exit;
    }
    memset(sm_p->u.io.contexts, 0,
           (target_datafile_count * sizeof(PINT_client_io_ctx)));

    sm_p->msgarray_count = target_datafile_count;
    sm_p->msgarray = (PINT_sm_msgpair_state *)malloc(
        (sm_p->msgarray_count * sizeof(PINT_sm_msgpair_state)));
    if (!sm_p->msgarray)
    {
        goto malloc_error_exit;
    }
    memset(sm_p->msgarray, 0, (sm_p->msgarray_count *
                               sizeof(PINT_sm_msgpair_state)));

    sm_p->u.io.total_cancellations_remaining = 0;

    /* initialize all per server I/O operation contexts and requests */
    for(i = 0; i < target_datafile_count; i++)
    {
        PINT_client_io_ctx *cur_ctx = &sm_p->u.io.contexts[i];
        PINT_sm_msgpair_state *msg = &sm_p->msgarray[i];

        assert(msg && cur_ctx);
        
        memset(cur_ctx, 0, sizeof(PINT_client_io_ctx));
        memset(msg, 0, sizeof(PINT_sm_msgpair_state));

        gossip_debug(GOSSIP_IO_DEBUG, "initializing context[%d] %p\n",
                     i, cur_ctx);

        cur_ctx->msg = msg;
        cur_ctx->index = i;
        cur_ctx->server_nr = sm_p->u.io.datafile_index_array[i];
        cur_ctx->data_handle =
            attr->u.meta.dfile_array[cur_ctx->server_nr];

        PINT_flow_reset(&cur_ctx->flow_desc);

        gossip_debug(GOSSIP_IO_DEBUG, "  filling I/O request "
                     "for %Lu\n", Lu(cur_ctx->data_handle));

        PINT_SERVREQ_IO_FILL(
            msg->req,
            *sm_p->cred_p,
            sm_p->object_ref.fs_id,
            cur_ctx->data_handle,
            sm_p->u.io.io_type,
            sm_p->u.io.flowproto_type,
            sm_p->u.io.datafile_index_array[i],
            attr->u.meta.dfile_count,
            attr->u.meta.dist,
            sm_p->u.io.file_req,
            sm_p->u.io.file_req_offset,
            PINT_REQUEST_TOTAL_BYTES(sm_p->u.io.mem_req));

        msg->fs_id = sm_p->object_ref.fs_id;
        msg->handle = cur_ctx->data_handle;
        msg->retry_flag = PVFS_MSGPAIR_NO_RETRY;
        msg->comp_fn = NULL;

        ret = PINT_cached_config_map_to_server(
               &msg->svr_addr, msg->handle, msg->fs_id);

        if (ret)
        {
            gossip_err("Failed to map meta server address\n");
            js_p->error_code = ret;
            return 1;
        }
    }

    sm_p->u.io.datafile_count = target_datafile_count;

    js_p->error_code = 0;
    return 1;

  malloc_error_exit:
    CLEAN_PRIVATE_MEMBERS(sm_p);

    js_p->error_code = -PVFS_ENOMEM;
    return 1;
}

/*
  This is based on msgpairarray_post() in msgpairarray.c.  It's
  different enough in that we don't have to wait on the msgpairarray
  operations to all complete before posting flows as we can for each
  server individually when we're ready.  this avoids the msgpairarray
  sync point implicit in the design
*/
static int io_datafile_post_msgpairs(PINT_client_sm *sm_p,
                                     job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, i = 0;
    unsigned long status_user_tag = 0;
    int must_loop_encodings = 0;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "io_datafile_post_msgpairs "
                 "state: post (%d message(s))\n", sm_p->msgarray_count);

    if (sm_p->op_cancelled)
    {
        js_p->error_code = -PVFS_ECANCEL;
        return 1;
    }

    js_p->error_code = 0;

    assert(sm_p->msgarray);
    assert(sm_p->msgarray_count == sm_p->u.io.datafile_count);

    /* completion count tracks sends/recvs separately */
    sm_p->u.io.msgpair_completion_count = (2 * sm_p->u.io.datafile_count);

    for(i = 0; i < sm_p->u.io.datafile_count; i++)
    {
        PINT_client_io_ctx *cur_ctx = &sm_p->u.io.contexts[i];
        PINT_sm_msgpair_state *msg = &sm_p->msgarray[i];

        assert(cur_ctx && msg);
        assert(cur_ctx->msg == msg);

        if (!ENCODING_IS_VALID(sm_p->u.io.encoding))
        {
            PRINT_ENCODING_ERROR("supported", sm_p->u.io.encoding);
            must_loop_encodings = 1;
            sm_p->u.io.encoding = (ENCODING_INVALID_MIN + 1);
        }
        else if (!ENCODING_IS_SUPPORTED(sm_p->u.io.encoding))
        {
            PRINT_ENCODING_ERROR("supported", sm_p->u.io.encoding);
            must_loop_encodings = 1;
            sm_p->u.io.encoding = ENCODING_SUPPORTED_MIN;
        }

      try_next_encoding:
        assert(ENCODING_IS_VALID(sm_p->u.io.encoding));

        ret = PINT_encode(&msg->req, PINT_ENCODE_REQ, &msg->encoded_req,
                          msg->svr_addr, sm_p->u.io.encoding);
        if (ret)
        {
            if (must_loop_encodings)
            {
                gossip_debug(GOSSIP_CLIENT_DEBUG, "Looping through "
                             "encodings [%d/%d]\n", sm_p->u.io.encoding,
                             ENCODING_INVALID_MAX);

                sm_p->u.io.encoding++;
                if (ENCODING_IS_VALID(sm_p->u.io.encoding))
                {
                    goto try_next_encoding;
                }
            }
            /*
              FIXME: make this a clean error transition by adjusting
              the completion count and/or (not) exiting
            */
            PVFS_perror_gossip("PINT_encode failed", ret);
            js_p->error_code = ret;
            return 1;
        }

        /* calculate maximum response message size and allocate it */
        msg->max_resp_sz = PINT_encode_calc_max_size(
            PINT_ENCODE_RESP, msg->req.op, sm_p->u.io.encoding);
        msg->encoded_resp_p = BMI_memalloc(
            msg->svr_addr, msg->max_resp_sz, BMI_RECV);
        if (!msg->encoded_resp_p)
        {
            /* FIXME: see above FIXME */
            js_p->error_code = -PVFS_ENOMEM;
            return 1;
        }

        /*
          recalculate the status user tag based on this the progress
          of the current context like this: status_user_tag = (4 *
          (context index) + context phase)
        */
        assert(cur_ctx->index == i);
        status_user_tag = ((4 * i) + IO_SM_PHASE_REQ_MSGPAIR_RECV);

        gossip_debug(GOSSIP_IO_DEBUG," posting recv with "
                     "status_user_tag=%lu (max_size %d)\n",
                     status_user_tag, msg->max_resp_sz);

        cur_ctx->session_tag = PINT_util_get_next_tag();

        ret = job_bmi_recv(
            msg->svr_addr, msg->encoded_resp_p, msg->max_resp_sz,
            cur_ctx->session_tag, BMI_PRE_ALLOC, sm_p, status_user_tag,
            &msg->recv_status, &msg->recv_id, pint_client_sm_context,
            PVFS2_CLIENT_JOB_TIMEOUT);

        cur_ctx->msg_recv_has_been_posted = 1;
        cur_ctx->msg_recv_in_progress = 1;

        if (ret == 0)
        {
            int tmp = 0;
            /* perform a quick test to see if the recv failed before
             * posting the send; if it reports an error quickly then
             * we can save the confusion of sending a request for
             * which we can't recv a response
             */
            ret = job_test(msg->recv_id, &tmp, NULL,
                           &msg->recv_status, 0,
                           pint_client_sm_context);
        }

        if ((ret < 0) || (ret == 1))
        {
            /*
              this recv can't complete yet without errors because we
              haven't sent the request yet
            */
            assert((ret < 0) || (msg->recv_status.error_code != 0));
            if (ret < 0)
            {
                PVFS_perror_gossip("Post of receive failed", ret);
            }
            else
            {
                PVFS_perror_gossip("Receive immediately failed",
                                   msg->recv_status.error_code);
            }

            PVFS_perror_gossip("recv completed before send failure", ret);

            cur_ctx->msg_recv_in_progress = 0;
            sm_p->u.io.msgpair_completion_count--;

            /* FIXME: see above FIXME */
            js_p->error_code = ret;
            return 1;
        }
        assert(ret == 0);

        assert(cur_ctx->index == i);
        status_user_tag = ((4 * i) + IO_SM_PHASE_REQ_MSGPAIR_SEND);

        gossip_debug(GOSSIP_IO_DEBUG," posting send with "
                     "status_user_tag=%lu\n", status_user_tag);

        ret = job_bmi_send_list(
            msg->encoded_req.dest, msg->encoded_req.buffer_list,
            msg->encoded_req.size_list, msg->encoded_req.list_count,
            msg->encoded_req.total_size, cur_ctx->session_tag,
            msg->encoded_req.buffer_type, 1, sm_p, status_user_tag,
            &msg->send_status, &msg->send_id, pint_client_sm_context,
            PVFS2_CLIENT_JOB_TIMEOUT);

        cur_ctx->msg_send_has_been_posted = 1;
        cur_ctx->msg_send_in_progress = 1;

        if ((ret < 0) ||
            ((ret == 1) && (msg->send_status.error_code != 0)))
        {
            if (ret < 0)
            {
                PVFS_perror_gossip("Post of send failed", ret);
            }
            else
            {
                PVFS_perror_gossip("Send immediately failed",
                    msg->recv_status.error_code);
            }

            msg->op_status = msg->send_status.error_code;
            msg->send_id = 0;

            /*
              cancel the recv and decrement the completion count, but
              still wait for the recv to complete
            */
            gossip_err("Send error: canceling recv.\n");

	    job_bmi_cancel(msg->recv_id, pint_client_sm_context);
            cur_ctx->msg_send_in_progress = 0;
            sm_p->u.io.msgpair_completion_count--;

            /* FIXME: see above FIXME */
        }
        else if (ret == 1)
        {
            gossip_debug(
                GOSSIP_IO_DEBUG, "  io_datafile_post_msgpairs: "
                "send completed immediately.\n");

            /* 0 is the valid "completed job id" value */
            msg->send_id = 0;

            cur_ctx->msg_send_in_progress = 0;
            sm_p->u.io.msgpair_completion_count--;

            if (msg->send_status.error_code != 0)
            {
                PVFS_perror_gossip("send status failure",
                                   msg->send_status.error_code);

                /* FIXME: see above FIXME */
                js_p->error_code = msg->send_status.error_code;
                return 1;
            }
        }
    }

    gossip_debug(GOSSIP_IO_DEBUG, "io_datafile_post_msgpairs: "
                 "completion count is %d\n",
                 sm_p->u.io.msgpair_completion_count);

    js_p->error_code = 0;
    return 0;
}

/*
  This state allows us to make sure all posted operations complete and
  are accounted for.  since this handles ALL operation completions,
  there's special case handling of completing the msgpair recv.  in
  this case we post the flow operations as soon as we see them (the
  main motivation for not using the common msgpairarray code).
*/
static int io_datafile_complete_operations(PINT_client_sm *sm_p,
                                           job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, index = 0;
    unsigned long status_user_tag = (unsigned long)
        js_p->status_user_tag;
    PINT_client_io_ctx *cur_ctx = NULL;
    int matched_send_or_recv = 0;

    gossip_debug(
        GOSSIP_CLIENT_DEBUG, "(%p) io_datafile_complete_operations "
        "(tag %lu)\n", sm_p, status_user_tag);

    if (js_p->error_code)
    {
        PVFS_perror_gossip("io_datafile_complete_operations failed",
                           js_p->error_code);
        /*
          if we looped back here following an error, we need to figure
          out what happened and cleanup properly
        */
        if ((sm_p->u.io.msgpair_completion_count == 0) &&
            (sm_p->u.io.flow_completion_count == 0) &&
            (sm_p->u.io.write_ack_completion_count == 0))
        {
            gossip_err("*** error path and all operations are "
                       "complete\n");

            /* preserve this error code for io_analyze_results */
            sm_p->u.io.stored_error_code = js_p->error_code;

            js_p->error_code = IO_DATAFILE_TRANSFERS_COMPLETE;
            return 1;
        }

        gossip_err("*** error path with %d msgpairs pending, %d flows "
                   "pending, %d write acks pending\n",
                   sm_p->u.io.msgpair_completion_count,
                   sm_p->u.io.flow_completion_count,
                   sm_p->u.io.write_ack_completion_count);
    }

    assert(sm_p->msgarray_count == sm_p->u.io.datafile_count);
    assert(sm_p->u.io.msgpair_completion_count > -1);
    assert(sm_p->u.io.flow_completion_count > -1);
    assert(sm_p->u.io.write_ack_completion_count > -1);

    /* check if we're completing a send or recv msgpair */
    if (STATUS_USER_TAG_IS_SEND_OR_RECV(status_user_tag))
    {
        assert(sm_p->u.io.msgpair_completion_count > -1);
        /*
          the completion count might be zero when recovering from a
          cancellation
        */
        if (sm_p->u.io.msgpair_completion_count)
        {
            ret = complete_context_send_or_recv(sm_p, js_p);
            if (ret < 0)
            {
                PVFS_perror_gossip(
                    "complete_context_send_or_recv failed", ret);
                js_p->error_code = ret;
                return 1;
            }
            else if (ret == 0)
            {
                gossip_debug(GOSSIP_IO_DEBUG, "  matched send in context "
                             "%d; continuing.\n", index);
                js_p->error_code = 0;
                return 0;
            }
            assert(ret == IO_RECV_COMPLETED);

            matched_send_or_recv = 1;
        }
    }

    /* if we've just completed a recv above, post the flow here */
    if (ret == IO_RECV_COMPLETED)
    {
        ret = process_context_recv_and_post_flow(sm_p, js_p);
        if (ret < 0)
        {
            char buf[64] = {0};
            PVFS_strerror_r(ret, buf, 64);

            gossip_err("process_context_recv_and_post_flow "
                       "failed: %s (%d remaining msgpairs)\n", buf,
                       sm_p->u.io.msgpair_completion_count);

            js_p->error_code = ret;
            return 1;
        }
    }

    /* check if we've completed all msgpairs and posted all flows */
    if (matched_send_or_recv)
    {
        if (sm_p->u.io.msgpair_completion_count == 0)
        {
            gossip_debug(GOSSIP_IO_DEBUG, "*** all msgpairs complete "
                         "(all flows posted)\n");
        }
        else
        {
            gossip_debug(
                GOSSIP_IO_DEBUG, "*** %d msgpair completions "
                "pending\n", sm_p->u.io.msgpair_completion_count);
        }
        return 0;
    }

    /* at this point, we're either completing a flow or a write ack */
    if (STATUS_USER_TAG_TYPE(status_user_tag, IO_SM_PHASE_FLOW))
    {
        assert(sm_p->u.io.flow_completion_count);

        index = STATUS_USER_TAG_GET_INDEX(
            status_user_tag, IO_SM_PHASE_FLOW);
        cur_ctx = &sm_p->u.io.contexts[index];
        assert(cur_ctx);

        cur_ctx->flow_status = *js_p;

        if (cur_ctx->write_ack_in_progress)
        {
            int ret = 0;

            assert(sm_p->u.io.write_ack_completion_count);
            ret = job_reset_timeout(cur_ctx->write_ack.recv_id,
                                    PVFS2_CLIENT_JOB_TIMEOUT);

            /*
              allow -PVFS_EINVAL errors in case the recv has already
              completed (before we've processed it)
            */
            assert((ret == 0) || (ret == -PVFS_EINVAL));
        }

        gossip_debug(GOSSIP_IO_DEBUG, "  matched completed flow for "
                     "context %p%s\n", cur_ctx,
                     ((cur_ctx->write_ack_in_progress ?
                       " and\n\treset write_recv timeout" : "")));

        cur_ctx->flow_in_progress = 0;
        sm_p->u.io.flow_completion_count--;
        assert(sm_p->u.io.flow_completion_count > -1);
    }
    else if (STATUS_USER_TAG_TYPE(status_user_tag, IO_SM_PHASE_FINAL_ACK))
    {
        assert(sm_p->u.io.write_ack_completion_count);

        index = STATUS_USER_TAG_GET_INDEX(
            status_user_tag, IO_SM_PHASE_FINAL_ACK);
        cur_ctx = &sm_p->u.io.contexts[index];
        assert(cur_ctx);

        assert(cur_ctx->write_ack.recv_status.actual_size <=
               cur_ctx->write_ack.max_resp_sz);

        cur_ctx->write_ack.recv_id = 0;
        cur_ctx->write_ack.recv_status = *js_p;

        gossip_debug(GOSSIP_IO_DEBUG, "  matched completed ack for "
                     "context %p\n", cur_ctx);

        cur_ctx->write_ack_in_progress = 0;
        sm_p->u.io.write_ack_completion_count--;
        assert(sm_p->u.io.write_ack_completion_count > -1);
    }

    js_p->error_code = 0;

    if ((sm_p->u.io.msgpair_completion_count == 0) &&
        (sm_p->u.io.flow_completion_count == 0) &&
        (sm_p->u.io.write_ack_completion_count == 0))
    {
        gossip_debug(GOSSIP_IO_DEBUG, "*** all operations %s "
                     "(msgpairs, flows, write acks)\n",
                     (sm_p->op_cancelled ? "cancelled" : "completed"));

        js_p->error_code = IO_DATAFILE_TRANSFERS_COMPLETE;
        return 1;
    }

    if (sm_p->op_cancelled)
    {
        gossip_debug(GOSSIP_IO_DEBUG, "detected I/O cancellation with "
                     "%d flow(s) and %d write ack(s) pending\n",
                     sm_p->u.io.flow_completion_count,
                     sm_p->u.io.write_ack_completion_count);
    }
    else
    {
        gossip_debug(GOSSIP_IO_DEBUG, " %d flows pending, %d write acks "
                     "pending\n", sm_p->u.io.flow_completion_count,
                     sm_p->u.io.write_ack_completion_count);
    }
    return 0;
}

static int io_analyze_results(PINT_client_sm *sm_p,
                              job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, i = 0, may_need_zero_fill = 0;
    PVFS_size tmp_size = 0;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: "
                 "io_analyze_results\n", sm_p);

    if (sm_p->u.io.continue_analysis)
    {
        sm_p->u.io.continue_analysis = 0;
        ret = sm_p->u.io.saved_ret;
        js_p->error_code = sm_p->u.io.saved_error_code;
        goto post_size_retrieval_continue_analysis;
    }

    if (js_p->error_code != IO_DATAFILE_TRANSFERS_COMPLETE)
    {
        ret = (sm_p->u.io.stored_error_code ?
               sm_p->u.io.stored_error_code :
               js_p->error_code);

        if (ret == 0)
        {
            ret = (sm_p->op_cancelled ? -PVFS_ECANCEL : -PVFS_EIO);
        }
    }
    else if (!sm_p->op_cancelled)
    {
        /*
          look through all the contexts for errors, saving the first
          one to return (if any) while adding up the size of the
          transfer (in case things actually completed).
        */
        assert(sm_p->msgarray_count == sm_p->u.io.datafile_count);
        for(i = 0; i < sm_p->u.io.datafile_count; i++)
        {
            PINT_client_io_ctx *cur_ctx = &sm_p->u.io.contexts[i];
            assert(cur_ctx);

            tmp_size = sm_p->u.io.total_size;

            ret = check_context_status(
                cur_ctx, sm_p->u.io.io_type, &sm_p->u.io.total_size);
            if (ret < 0)
            {
                if (ret == -PVFS_ECANCEL)
                {
                    gossip_debug(GOSSIP_IO_DEBUG, "*** I/O operation "
                                 "cancelled\n");
                }
                else
                {
                    PVFS_perror_gossip(
                        "check_context_status found error", ret);
                }
                break;
            }

            /* if a context transferred 0 bytes, we may have a hole */
            if (tmp_size == sm_p->u.io.total_size)
            {
                may_need_zero_fill = 1;
            }

            gossip_debug(
                GOSSIP_IO_DEBUG, "[%d/%d] running size is %Ld\n",
                (i + 1), sm_p->u.io.datafile_count,
                Ld(sm_p->u.io.total_size));
        }

        /*
          at this point, we may know an error occurred.  if we
          couldn't find any errors in the context, use the preserved
          error code from the complete_operations state (which may be
          success)
        */
        if (ret == 0)
        {
            char buf[64] = {0};

            ret = (sm_p->op_cancelled ? -PVFS_ECANCEL :
                   sm_p->u.io.stored_error_code);

            PVFS_strerror_r(ret, buf, 64);
            gossip_debug(GOSSIP_IO_DEBUG, "no context errors found; "
                         "using: %s\n", buf);
        }

        /*
          if we got a short read, we need to process the request to
          look for zero fills of potential holes; alternatively, if
          we're sure we got all of the data we're looking for, don't
          bother with this special case
        */
        may_need_zero_fill =
            ((sm_p->u.io.total_size != PINT_REQUEST_TOTAL_BYTES(
                  sm_p->u.io.mem_req)) ? 1 : 0);

        /*
          if the total size read is 0, then it's not a hole (unless
          only a single server was contacted for data)
        */
        if ((ret == 0) && (sm_p->u.io.io_type == PVFS_IO_READ) &&
            (may_need_zero_fill == 1) &&
            ((sm_p->u.io.total_size > 0) || sm_p->msgarray_count == 1) &&
            PINT_REQUEST_NUM_CONTIG(sm_p->u.io.mem_req) &&
            PINT_REQUEST_NUM_CONTIG(sm_p->u.io.file_req))
        {
            if (check_for_zero_fill_on_read(sm_p) == IO_GET_DATAFILE_SIZE)
            {
                /*
                  we have to get the size for a trailing zero-fill;
                  save state to continue processing from here
                  afterward
                */
                sm_p->u.io.continue_analysis = 1;
                sm_p->u.io.saved_ret = ret;
                sm_p->u.io.saved_error_code = js_p->error_code;

                js_p->error_code = IO_GET_DATAFILE_SIZE;
                return 1;
            }

          post_size_retrieval_continue_analysis:
            /*
              flows are not reset until after checking for a possible
              zero fill, so if we're sure that's not possible, reset
              them here (otherwise they're reset after the trailing
              zero fill adjustment above following the
              IO_GET_DATAFILE_SIZE state)
            */
            for(i = 0; i < sm_p->u.io.datafile_count; i++)
            {
                PINT_client_io_ctx *cur_ctx = &sm_p->u.io.contexts[i];
                assert(cur_ctx);

                PINT_flow_reset(&cur_ctx->flow_desc);
            }
        }
    }
    else
    {
        ret = (sm_p->op_cancelled ? -PVFS_ECANCEL : -PVFS_EIO);
    }

    /* be sure there are no jobs still laying around */
    assert((sm_p->u.io.msgpair_completion_count == 0) &&
           (sm_p->u.io.flow_completion_count == 0) &&
           (sm_p->u.io.write_ack_completion_count == 0));

    /*
      FIXME: non bmi errors pop out in flow failures above -- they are
      not properly marked as flow errors either, so we check for them
      explicitly here (but not all -- fix it for real).
    */
    if (((PVFS_ERROR_CLASS(-ret) == PVFS_ERROR_BMI) ||
         (PVFS_ERROR_CLASS(-ret) == PVFS_ERROR_FLOW) ||
         (ret == -ECONNRESET) || (ret == -PVFS_EPROTO)) &&
        (sm_p->u.io.retry_count < PVFS2_CLIENT_RETRY_LIMIT))
    {
        if (sm_p->acache_hit)
        {
            PINT_acache_release(sm_p->pinode);
            sm_p->acache_hit = 0;
            sm_p->pinode = NULL;
        }

        assert(!sm_p->op_cancelled);

        sm_p->u.io.stored_error_code = 0;
        sm_p->u.io.retry_count++;

        gossip_debug(GOSSIP_IO_DEBUG, "Retrying I/O operation "
                     "(attempt number %d)\n", sm_p->u.io.retry_count);

        js_p->error_code = IO_RETRY;
        return 1;
    }

    gossip_debug(GOSSIP_IO_DEBUG, "total bytes transferred is %Ld\n",
                 Ld(sm_p->u.io.total_size));

    sm_p->u.io.io_resp_p->total_completed = sm_p->u.io.total_size;
    js_p->error_code = ret;

    return 1;
}

static int io_datafile_size_setup_msgpairs(
    PINT_client_sm *sm_p, job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, i = 0;
    PVFS_object_attr *attr = NULL;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: io_datafile_"
                 "size_setup_msgpairs\n", sm_p);

    js_p->error_code = 0;

    attr = (sm_p->acache_hit ? &sm_p->pinode->attr : &sm_p->acache_attr);
    assert(attr);

    if (sm_p->msgarray && (sm_p->msgarray != &sm_p->msgpair))
    {
        free(sm_p->msgarray);
    }
    sm_p->msgarray = (PINT_sm_msgpair_state *)malloc(
        attr->u.meta.dfile_count * sizeof(PINT_sm_msgpair_state));
    if (!sm_p->msgarray)
    {
        js_p->error_code = -PVFS_ENOMEM;
        return 1;
    }

    sm_p->u.io.size_array = (PVFS_size *)malloc(
        attr->u.meta.dfile_count * sizeof(PVFS_size));
    if (!sm_p->u.io.size_array)
    {
        free(sm_p->msgarray);
        sm_p->msgarray = NULL;

        js_p->error_code = -PVFS_ENOMEM;
        return 1;
    }

    sm_p->msgarray_count = attr->u.meta.dfile_count;

    for(i = 0; i < sm_p->msgarray_count; i++)
    {
        PINT_sm_msgpair_state *msg_p = &sm_p->msgarray[i];
        assert(msg_p);

        gossip_debug(GOSSIP_IO_DEBUG,
                     "  getting size for handle %Lu\n",
                     Lu(attr->u.meta.dfile_array[i]));

        PINT_SERVREQ_GETATTR_FILL(
            msg_p->req,
            *sm_p->cred_p,
            sm_p->object_ref.fs_id,
            attr->u.meta.dfile_array[i],
            PVFS_ATTR_DATA_SIZE);

        msg_p->fs_id = sm_p->object_ref.fs_id;
        msg_p->handle = attr->u.meta.dfile_array[i];
        msg_p->retry_flag = PVFS_MSGPAIR_RETRY;
        msg_p->comp_fn = io_datafile_size_comp_fn;
    }

    ret = PINT_serv_msgpairarray_resolve_addrs(
        sm_p->msgarray_count, sm_p->msgarray);
    if (ret < 0)
    {
        gossip_lerr("Error: failed to resolve meta server addresses.\n");
        js_p->error_code = ret;
    }
    return 1;
}

static int io_datafile_size_failure(
    PINT_client_sm *sm_p, job_status_s *js_p)
{
    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: io_datafile_"
                 "size_failure\n", sm_p);

    return 0;
}

static int io_analyze_size_results(
    PINT_client_sm *sm_p, job_status_s *js_p)
{
    int ret = -PVFS_EINVAL;
    PINT_dist *dist = NULL;
    PVFS_object_attr *attr = (sm_p->acache_hit ?
                              &sm_p->pinode->attr :
                              &sm_p->acache_attr);
    PVFS_size zero_fill_size = 0;

    gossip_debug(GOSSIP_CLIENT_DEBUG, "(%p) io state: io_analyze"
                 "size_results\n", sm_p);

    assert(attr && attr->u.meta.dist);

    dist = attr->u.meta.dist;
    assert(dist->methods);

    ret = PINT_dist_lookup(dist);
    assert(ret == 0);

    assert(sm_p->u.io.size_array);

    sm_p->u.io.size = (dist->methods->logical_file_size)(
        dist->params, attr->u.meta.dfile_count,
        sm_p->u.io.size_array);

    free(sm_p->u.io.size_array);
    sm_p->u.io.size_array = NULL;

    gossip_debug(GOSSIP_IO_DEBUG, " *** computed logical size of %Ld\n",
                 Ld(sm_p->u.io.size));

    zero_fill_size = PVFS_util_min(
        (sm_p->u.io.size - sm_p->u.io.file_req_offset -
         sm_p->u.io.total_size),
        (PINT_REQUEST_TOTAL_BYTES(sm_p->u.io.mem_req) -
         sm_p->u.io.total_size));

    if ((sm_p->u.io.file_req_offset + zero_fill_size) > sm_p->u.io.size)
    {
        zero_fill_size = (sm_p->u.io.size - sm_p->u.io.file_req_offset);
    }

    if (zero_fill_size > -1)
    {
        gossip_debug(GOSSIP_IO_DEBUG, "computed zero fill size of %Ld "
                     "at memory offset %Ld\n", Ld(zero_fill_size),
                     Ld(sm_p->u.io.total_size));

        memset((char *)(sm_p->u.io.buffer + sm_p->u.io.total_size),
               0, zero_fill_size);

        sm_p->u.io.total_size += zero_fill_size;
    }

    js_p->error_code = 0;
    return 1;
}

static int io_cleanup(PINT_client_sm *sm_p,
                      job_status_s *js_p)
{
    gossip_debug(GOSSIP_CLIENT_DEBUG,
                 "(%p) io state: io_cleanup\n", sm_p);

    CLEAN_PRIVATE_MEMBERS(sm_p);

    if (sm_p->acache_hit)
    {
        if (!sm_p->u.io.size)
        {
            gossip_debug(GOSSIP_CLIENT_DEBUG, "sys-io.sm Invalidating "
                         "cached attribute size\n");

            sm_p->pinode->attr.mask &=
                ~(PVFS_ATTR_DATA_SIZE | PVFS_ATTR_DATA_ALL);
        }
        else
        {
            sm_p->pinode->size = sm_p->u.io.size;
        }
        PINT_acache_set_valid(sm_p->pinode);

        PINT_acache_release(sm_p->pinode);
        sm_p->acache_hit = 0;
        sm_p->pinode = NULL;
    }
    else
    {
        PINT_free_object_attr(&sm_p->acache_attr);
    }

    sm_p->error_code = js_p->error_code;

    if (sm_p->error_code)
    {
        char buf[64] = {0};

        PVFS_strerror_r(sm_p->error_code, buf, 64);
        gossip_debug(GOSSIP_IO_DEBUG,
                     "*** Final I/O operation error is %s\n", buf);
    }

    sm_p->op_complete = 1;
    return 0;
}

/********************************************************************/

/*
  returns 0 on send completion; IO_RECV_COMPLETED on recv completion,
  and -PVFS_error on failure
*/
static inline int complete_context_send_or_recv(
    PINT_client_sm *sm_p,
    job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, index = 0;
    unsigned long status_user_tag = 0;
    PINT_client_io_ctx *cur_ctx = NULL;
    PINT_sm_msgpair_state *msg = NULL;

    gossip_debug(GOSSIP_IO_DEBUG,
                 "- complete_context_send_or_recv called\n");

    assert(sm_p && js_p);
    assert(sm_p->op == PVFS_SYS_IO);

    status_user_tag = (unsigned long)js_p->status_user_tag;

    if (STATUS_USER_TAG_TYPE(
            status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_RECV))
    {
        index = STATUS_USER_TAG_GET_INDEX(
            status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_RECV);

        gossip_debug(GOSSIP_IO_DEBUG, "got a recv completion with "
                     "context index %d\n", index);

        cur_ctx = &sm_p->u.io.contexts[index];
        assert(cur_ctx);

        msg = &sm_p->msgarray[index];
        msg->recv_id = 0;
        msg->recv_status = *js_p;

        assert(msg->recv_status.error_code <= 0);
        assert(msg->recv_status.actual_size <= msg->max_resp_sz);

        cur_ctx->msg_recv_in_progress = 0;
        sm_p->u.io.msgpair_completion_count--;

        ret = IO_RECV_COMPLETED;
    }
    else if (STATUS_USER_TAG_TYPE(
                 status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_SEND))
    {
        index = STATUS_USER_TAG_GET_INDEX(
            status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_RECV);

        gossip_debug(GOSSIP_IO_DEBUG, "got a send completion with "
                     "context index %d\n", index);

        cur_ctx = &sm_p->u.io.contexts[index];
        assert(cur_ctx);

        msg = &sm_p->msgarray[index];
        msg->send_id = 0;
        msg->send_status = *js_p;

        assert(msg->send_status.error_code <= 0);

        cur_ctx->msg_send_in_progress = 0;
        sm_p->u.io.msgpair_completion_count--;

        ret = 0;
    }
    return ret;
}

static inline int process_context_recv(
    PINT_client_io_ctx *cur_ctx,
    struct PINT_decoded_msg *decoded_resp,
    struct PVFS_server_resp **resp)
{
    int ret = -PVFS_EINVAL;

    gossip_debug(GOSSIP_IO_DEBUG, "- process_context_recv called\n");

    assert(cur_ctx && cur_ctx->msg && decoded_resp && resp);

    ret = PINT_serv_decode_resp(
        cur_ctx->msg->fs_id, cur_ctx->msg->encoded_resp_p, decoded_resp,
        &cur_ctx->msg->svr_addr,
        cur_ctx->msg->recv_status.actual_size, resp);

    if (ret)
    {
        PVFS_perror("PINT_server_decode_resp failed", ret);
        return ret;
    }

    assert((*resp)->status < 1);
    cur_ctx->msg->op_status = (*resp)->status;

    if (cur_ctx->msg->recv_status.error_code || cur_ctx->msg->op_status)
    {
        gossip_debug(
            GOSSIP_IO_DEBUG, "  error %d with status %d related "
            "to response from context %p; not submitting flow.\n",
            cur_ctx->msg->recv_status.error_code,
            cur_ctx->msg->op_status, cur_ctx);

        if (cur_ctx->msg->recv_status.error_code)
        {
            PVFS_perror_gossip(
                "process_context_recv (recv_status.error_code)",
                cur_ctx->msg->recv_status.error_code);
            ret = cur_ctx->msg->recv_status.error_code;
        }
        else if (cur_ctx->msg->op_status)
        {
            PVFS_perror_gossip("process_context_recv (op_status)",
                               cur_ctx->msg->op_status);
            ret = cur_ctx->msg->op_status;
        }

        PINT_serv_free_msgpair_resources(
            &cur_ctx->msg->encoded_req, cur_ctx->msg->encoded_resp_p,
            decoded_resp, &cur_ctx->msg->svr_addr,
            cur_ctx->msg->max_resp_sz);
    }
    return ret;
}

static inline int build_context_flow(
    PINT_client_sm *sm_p,
    PINT_client_io_ctx *cur_ctx,
    PVFS_object_attr *attr,
    struct PVFS_server_resp *resp)
{
    gossip_debug(GOSSIP_IO_DEBUG, "- build_context_flow called\n");

    if (!sm_p || !cur_ctx || !attr || !resp)
    {
        return -PVFS_EINVAL;
    }

    gossip_debug(GOSSIP_IO_DEBUG, "* mem req size is %Ld, "
                 "file_req size is %Ld (bytes)\n",
                 Ld(PINT_REQUEST_TOTAL_BYTES(sm_p->u.io.mem_req)),
                 Ld(PINT_REQUEST_TOTAL_BYTES(sm_p->u.io.file_req)));

    cur_ctx->flow_desc.file_data.fsize = resp->u.io.bstream_size;
    cur_ctx->flow_desc.file_data.dist = attr->u.meta.dist;
    cur_ctx->flow_desc.file_data.server_nr = cur_ctx->server_nr;
    cur_ctx->flow_desc.file_data.server_ct = attr->u.meta.dfile_count;

    cur_ctx->flow_desc.file_req = sm_p->u.io.file_req;
    cur_ctx->flow_desc.file_req_offset = sm_p->u.io.file_req_offset;

    cur_ctx->flow_desc.mem_req = sm_p->u.io.mem_req;

    cur_ctx->flow_desc.tag = cur_ctx->session_tag;
    cur_ctx->flow_desc.type = sm_p->u.io.flowproto_type;
    cur_ctx->flow_desc.user_ptr = NULL;

    gossip_debug(GOSSIP_IO_DEBUG, "   bstream_size = %Ld, datafile "
                 "nr=%d, ct=%d\n\t\t\tfile_req_off = %Ld\n",
                 Ld(cur_ctx->flow_desc.file_data.fsize),
                 cur_ctx->flow_desc.file_data.server_nr,
                 cur_ctx->flow_desc.file_data.server_ct,
                 Ld(cur_ctx->flow_desc.file_req_offset));

    if (sm_p->u.io.io_type == PVFS_IO_READ)
    {
        cur_ctx->flow_desc.file_data.extend_flag = 0;
        cur_ctx->flow_desc.src.endpoint_id = BMI_ENDPOINT;
        cur_ctx->flow_desc.src.u.bmi.address = cur_ctx->msg->svr_addr;
        cur_ctx->flow_desc.dest.endpoint_id = MEM_ENDPOINT;
        cur_ctx->flow_desc.dest.u.mem.buffer = sm_p->u.io.buffer;
    }
    else
    {
        assert(sm_p->u.io.io_type == PVFS_IO_WRITE);

        cur_ctx->flow_desc.file_data.extend_flag = 1;
        cur_ctx->flow_desc.src.endpoint_id = MEM_ENDPOINT;
        cur_ctx->flow_desc.src.u.mem.buffer = sm_p->u.io.buffer;
        cur_ctx->flow_desc.dest.endpoint_id = BMI_ENDPOINT;
        cur_ctx->flow_desc.dest.u.bmi.address = cur_ctx->msg->svr_addr;
    }
    return 0;
}

static inline int process_context_recv_and_post_flow(
    PINT_client_sm *sm_p,
    job_status_s *js_p)
{
    int ret = -PVFS_EINVAL, index = 0;
    unsigned long status_user_tag = 0;
    struct PINT_decoded_msg decoded_resp;
    struct PVFS_server_resp *resp = NULL;
    PVFS_object_attr *attr = NULL;
    PINT_client_io_ctx *cur_ctx = NULL;

    gossip_debug(GOSSIP_IO_DEBUG,
                 "- process_context_recv_and_post_flow called\n");

    assert(sm_p && js_p);
    assert(STATUS_USER_TAG_TYPE(
               status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_RECV));

    status_user_tag = (unsigned long)js_p->status_user_tag;

    index = STATUS_USER_TAG_GET_INDEX(
        status_user_tag, IO_SM_PHASE_REQ_MSGPAIR_RECV);

    cur_ctx = &sm_p->u.io.contexts[index];
    assert(cur_ctx && cur_ctx->msg);

    if (js_p->error_code)
    {
        PVFS_perror_gossip("pre-process_context_recv failed",
                           js_p->error_code);
        return js_p->error_code;
    }

    ret = process_context_recv(cur_ctx, &decoded_resp, &resp);
    if (ret)
    {
        PVFS_perror_gossip("process_context_recv failed", ret);
        return ret;
    }

    attr = (sm_p->acache_hit ? &sm_p->pinode->attr : &sm_p->acache_attr);
    assert(attr);

    ret = build_context_flow(sm_p, cur_ctx, attr, resp);
    if (ret < 0)
    {
        PVFS_perror_gossip("build_context_flow failed", ret);
        return ret;
    }

    ret = PINT_serv_free_msgpair_resources(
        &cur_ctx->msg->encoded_req, cur_ctx->msg->encoded_resp_p,
        &decoded_resp, &cur_ctx->msg->svr_addr,
        cur_ctx->msg->max_resp_sz);

    if (ret)
    {
        PVFS_perror_gossip("PINT_serv_free_msgpair_resources "
                           "failed", ret);
        return ret;
    }

    if (sm_p->u.io.io_type == PVFS_IO_WRITE)
    {
        gossip_debug(GOSSIP_IO_DEBUG, "  preposting write "
                     "ack for context %p.\n", cur_ctx);

        cur_ctx->write_ack.max_resp_sz = PINT_encode_calc_max_size(
            PINT_ENCODE_RESP, PVFS_SERV_WRITE_COMPLETION,
            sm_p->u.io.encoding);
        cur_ctx->write_ack.encoded_resp_p = BMI_memalloc(
            cur_ctx->msg->svr_addr, cur_ctx->write_ack.max_resp_sz,
            BMI_RECV);

        if (!cur_ctx->write_ack.encoded_resp_p)
        {
            gossip_err("BMI_memalloc (for write ack) failed\n");
            return -PVFS_ENOMEM;
        }

        /*
          we're pre-posting the final write ack here, even though it's
          ahead of the flow phase; reads are at the flow phase.

          the timeout used here is a scaling one that needs to be long
          enough for the entire flow to occur
        */
        status_user_tag = ((4 * cur_ctx->index) + IO_SM_PHASE_FINAL_ACK);

        /*
          pre-post this recv with an infinite timeout and adjust it
          after the flow completes since we don't know how long a flow
          can take at this point
        */ 
        ret = job_bmi_recv(
            cur_ctx->msg->svr_addr, cur_ctx->write_ack.encoded_resp_p,
            cur_ctx->write_ack.max_resp_sz, cur_ctx->session_tag,
            BMI_PRE_ALLOC, sm_p, status_user_tag,
            &cur_ctx->write_ack.recv_status, &cur_ctx->write_ack.recv_id,
            pint_client_sm_context, JOB_TIMEOUT_INF);

        if (ret < 0)
        {
            gossip_err("job_bmi_recv (write ack) failed\n");
            return ret;
        }

        assert(ret == 0);
        cur_ctx->write_ack_has_been_posted = 1;
        cur_ctx->write_ack_in_progress = 1;
        sm_p->u.io.write_ack_completion_count++;
    }

    status_user_tag = ((4 * cur_ctx->index) + IO_SM_PHASE_FLOW);

    ret = job_flow(
        &cur_ctx->flow_desc, sm_p, status_user_tag,
        &cur_ctx->flow_status, &cur_ctx->flow_job_id,
        pint_client_sm_context, PVFS2_CLIENT_JOB_TIMEOUT);

    if (ret < 0)
    {
        gossip_err("job_flow failed\n");
        return ret;
    }
    else if (ret == 1)
    {
        gossip_debug(GOSSIP_IO_DEBUG, "  flow for context %p "
                     "completed immediately\n", cur_ctx);
        assert(cur_ctx->flow_status.error_code == 0);
    }
    else
    {
        gossip_debug(GOSSIP_IO_DEBUG, "  posted flow for "
                     "context %p\n", cur_ctx);

        cur_ctx->flow_has_been_posted = 1;
        cur_ctx->flow_in_progress = 1;
        sm_p->u.io.flow_completion_count++;
    }
    return ret;
}

static inline int check_context_status(
    PINT_client_io_ctx *cur_ctx,
    int io_type,
    PVFS_size *total_size)
{
    int ret = 0;

    gossip_debug(GOSSIP_IO_DEBUG, "- check_context_status called\n");

    assert(cur_ctx && cur_ctx->msg && total_size);

    if (cur_ctx->msg->send_status.error_code)
    {
        gossip_debug(GOSSIP_IO_DEBUG,
                     "  error (%d) in msgpair send for context %p\n",
                     cur_ctx->msg->send_status.error_code, cur_ctx);

        assert(cur_ctx->msg_send_has_been_posted);
        ret = cur_ctx->msg->send_status.error_code;
    }
    else if (cur_ctx->msg->recv_status.error_code)
    {
        gossip_debug(GOSSIP_IO_DEBUG,
                     "  error (%d) in msgpair recv for context %p\n",
                     cur_ctx->msg->recv_status.error_code, cur_ctx);

        assert(cur_ctx->msg_recv_has_been_posted);
        ret = cur_ctx->msg->recv_status.error_code;
    }
    else if (cur_ctx->flow_status.error_code)
    {
        gossip_debug(GOSSIP_IO_DEBUG,
                     "  error (%d) in flow for context %p\n",
                     cur_ctx->flow_status.error_code, cur_ctx);

        assert(cur_ctx->flow_has_been_posted);
        PINT_flow_reset(&cur_ctx->flow_desc);
        ret = cur_ctx->flow_status.error_code;
    }
    else if (io_type == PVFS_IO_READ)
    {
        gossip_debug(
            GOSSIP_IO_DEBUG, "  %Ld bytes read from context %p\n",
            Ld(cur_ctx->flow_desc.total_transfered), cur_ctx);

        /* size for reads are reported in the flow */
        *total_size += cur_ctx->flow_desc.total_transfered;

        /*
          we can't reset the flow here in case we have to do a zero
          fill adjustment that we haven't detected yet
        */
    }
    else if (io_type == PVFS_IO_WRITE)
    {
        if (cur_ctx->write_ack.recv_status.error_code)
        {
            gossip_debug(
                GOSSIP_IO_DEBUG,
                "  error (%d) in final ack for context %p\n",
                cur_ctx->write_ack.recv_status.error_code, cur_ctx);

            assert(cur_ctx->write_ack_has_been_posted);
            ret = cur_ctx->write_ack.recv_status.error_code;
        }
        else if (cur_ctx->write_ack_has_been_posted)
        {
            struct PINT_decoded_msg decoded_resp;
            struct PVFS_server_resp *resp = NULL;
            /*
              size for writes are reported in the final ack, but we
              have to decode it first
            */
            ret = PINT_serv_decode_resp(
                cur_ctx->msg->fs_id, cur_ctx->write_ack.encoded_resp_p,
                &decoded_resp, &cur_ctx->msg->svr_addr,
                cur_ctx->write_ack.recv_status.actual_size, &resp);
            if (ret == 0)
            {
                gossip_debug(
                    GOSSIP_IO_DEBUG,
                    "  %Ld bytes written to context %p\n",
                    Ld(resp->u.write_completion.total_completed),
                    cur_ctx);

                *total_size += resp->u.write_completion.total_completed;

                PINT_decode_release(&decoded_resp, PINT_DECODE_RESP);
            }
            else
            {
                PVFS_perror_gossip("PINT_serv_decode_resp failed", ret);
            }

            PINT_flow_reset(&cur_ctx->flow_desc);
            BMI_memfree(cur_ctx->msg->svr_addr,
                        cur_ctx->write_ack.encoded_resp_p,
                        cur_ctx->write_ack.max_resp_sz, BMI_RECV);
        }
    }
    return ret;
}

static inline int check_for_zero_fill_on_read(PINT_client_sm *sm_p)
{
    int i = 0, val = 0;
    PINT_Request_state *mrs = NULL, *frs = NULL;
    PINT_Request_result seg_result;
    PINT_Request_file_data *file_data = NULL;
    int64_t offset_array[16] = {0}, size_array[16] = {0};
    PVFS_size total_ctx_size = 0;

    assert(sm_p->u.io.io_type == PVFS_IO_READ);

    mrs = PINT_New_request_state(sm_p->u.io.mem_req);
    frs = PINT_New_request_state(sm_p->u.io.file_req);
    assert(mrs && frs);

    memset(&seg_result, 0, sizeof(PINT_Request_result));
    seg_result.bytemax = 4194304;
    seg_result.segmax = 16;
    seg_result.offset_array = offset_array;
    seg_result.size_array = size_array;

    assert(sm_p->msgarray_count == sm_p->u.io.datafile_count);
    for(i = 0; i < sm_p->u.io.datafile_count; i++)
    {
        int num_req_iterations = 0;
        PINT_client_io_ctx *cur_ctx = &sm_p->u.io.contexts[i];
        assert(cur_ctx);

        file_data = &cur_ctx->flow_desc.file_data;
        assert(file_data);

        PINT_REQUEST_STATE_RESET(mrs);
        PINT_REQUEST_STATE_RESET(frs);

        val = 0;
        total_ctx_size = 0;
        num_req_iterations = 0;

        /*
          first make sure we're not complete before doing a deeper
          analysis, as it's possible to be complete in the case of the
          last of a trailing zero fill.  we'll know this is the case
          if the file req offset is bigger than the bstream size, but
          unfortunately it's expensive since we need the accurate file
          size to know if more zero fills are necessary at this point
        */
        if (cur_ctx->flow_desc.file_req_offset >
            cur_ctx->flow_desc.file_data.fsize)
        {        
            gossip_debug(GOSSIP_IO_DEBUG, "[1] Checking for read past EOF "
                         "(size=%Ld, total_size=%Ld)\n",
                         Ld(sm_p->u.io.size), Ld(sm_p->u.io.total_size));

            PINT_Free_request_state(mrs);
            PINT_Free_request_state(frs);

            return IO_GET_DATAFILE_SIZE;
        }

        /*
          process the request to figure out how much we *should* have
          read
        */
        do
        {
            seg_result.bytes = 0;
            seg_result.segs = 0;

            memset(offset_array, 0, (16 * sizeof(int64_t)));
            memset(size_array, 0, (16 * sizeof(int64_t)));

            val = PINT_Process_request(
                frs, mrs, file_data, &seg_result, PINT_CLIENT);

            if (val < 0)
            {
                gossip_err("Failed to process request; no chance of "
                           "zero fill\n");
                goto request_process_failure;
            }

            total_ctx_size += seg_result.bytes;
            num_req_iterations++;

        } while(!PINT_REQUEST_DONE(frs) && (val > -1));

        gossip_debug(
            GOSSIP_IO_DEBUG, "***** context[%d] read %Ld of %Ld "
            "bytes [%d iterations]\n", cur_ctx->index,
            Ld(cur_ctx->flow_desc.total_transfered),
            Ld(total_ctx_size), num_req_iterations);

        if ((total_ctx_size > 0) &&
            (total_ctx_size != cur_ctx->flow_desc.total_transfered))
        {
            PVFS_size zero_fill_size =
                (total_ctx_size -
                 cur_ctx->flow_desc.total_transfered);
            PVFS_size zero_fill_offset = sm_p->u.io.total_size;

            /*
              if after zero filling, we'll have satisfied more than
              the requested amount (or will have moved past the
              logical EOF), be sure to truncate/adjust the size down
            */
            if ((sm_p->u.io.total_size + zero_fill_size) >
                PINT_REQUEST_TOTAL_BYTES(sm_p->u.io.mem_req))
            {
                zero_fill_size = PINT_REQUEST_TOTAL_BYTES(
                    sm_p->u.io.mem_req) - sm_p->u.io.total_size;
            }
            else if (sm_p->u.io.size &&
                     (cur_ctx->flow_desc.file_req_offset +
                      zero_fill_size) > sm_p->u.io.size)
            {
                zero_fill_size = sm_p->u.io.size -
                    cur_ctx->flow_desc.file_req_offset;
            }
            else if (total_ctx_size !=
                     cur_ctx->flow_desc.total_transfered)
            {
                goto check_for_trailing_hole;
            }
            gossip_debug(GOSSIP_IO_DEBUG, "zero_fill_size is %Ld | "
                         "sm_p->u.io.size is %Ld\n", zero_fill_size,
                         sm_p->u.io.size);

            if (zero_fill_size > -1)
            {
                gossip_debug(GOSSIP_IO_DEBUG, " SHORT read -- "
                             "zero filling %Ld bytes at offset %Ld\n",
                             Ld(zero_fill_size), Ld(zero_fill_offset));

                memset((char *)(cur_ctx->flow_desc.dest.u.mem.buffer +
                                zero_fill_offset), 0, zero_fill_size);

                sm_p->u.io.total_size += zero_fill_size;

                /* exit loop -- we're done zero-filling */
                break;
            }
        }
        else if (total_ctx_size != cur_ctx->flow_desc.total_transfered)
        {
          check_for_trailing_hole:
            /*
              if there is a trailing file hole (i.e. a hole after
              valid data until EOF), handle that case here by zero
              filling based on how big the file *should* be
            */
            gossip_debug(
                GOSSIP_IO_DEBUG, "[2] Checking for read past EOF "
                "(size=%Ld, total_size=%Ld)\n",
                Ld(sm_p->u.io.size), Ld(sm_p->u.io.total_size));

            PINT_Free_request_state(mrs);
            PINT_Free_request_state(frs);

            return IO_GET_DATAFILE_SIZE;
        }
    }

  request_process_failure:

    PINT_Free_request_state(mrs);
    PINT_Free_request_state(frs);
    return 0;
}

/*
  determines what subset of the datafiles actually contain data that
  we are interested in for this request. returns 0 on success,
  -PVFS_error on failure
*/
static int io_find_target_datafiles(
    PVFS_Request mem_req,
    PVFS_Request file_req,
    PVFS_offset file_req_offset,
    PINT_dist *dist_p,
    PVFS_handle *input_handle_array,
    int input_handle_count,
    int *handle_index_array,
    int *handle_index_out_count)
{
    int ret = -PVFS_EINVAL, i = 0;
    struct PINT_Request_state *req_state = NULL;
    struct PINT_Request_state *mem_req_state = NULL;
    PINT_Request_file_data tmp_file_data;
    PINT_Request_result tmp_result;

    gossip_debug(GOSSIP_IO_DEBUG, "- io_find_target_datafiles called\n");

    if (!handle_index_array || !handle_index_out_count)
    {
        return ret;
    }
    *handle_index_out_count = 0;

    req_state = PINT_New_request_state(file_req);
    if (!req_state)
    {
        return -PVFS_ENOMEM;
    }
    mem_req_state = PINT_New_request_state(mem_req);
    if (!mem_req_state)
    {
        PINT_Free_request_state(req_state);
        return -PVFS_ENOMEM;
    }

    tmp_file_data.dist = dist_p;
    tmp_file_data.server_ct = input_handle_count;
    tmp_file_data.extend_flag = 1;

    for(i = 0; i < input_handle_count; i++)
    {
        /* NOTE: we don't have to give an accurate file size here, as
         * long as we set the extend flag to tell the I/O req
         * processor to continue past eof if needed
         */
        tmp_file_data.fsize = 0;  
        tmp_file_data.server_nr = i;

        PINT_REQUEST_STATE_RESET(req_state);
        PINT_REQUEST_STATE_RESET(mem_req_state);

        /* if a file datatype offset was specified, go ahead and skip
         * ahead before calculating
         */
        if (file_req_offset)
        {
            PINT_REQUEST_STATE_SET_TARGET(req_state, file_req_offset);
        }

        PINT_REQUEST_STATE_SET_FINAL(req_state,
            file_req_offset+PINT_REQUEST_TOTAL_BYTES(mem_req));

        memset(&tmp_result, 0, sizeof(PINT_Request_result));
        tmp_result.bytemax = 1;
        tmp_result.segmax = 1;

        /* PINT_Process_request() returns number of bytes processed */
        ret = PINT_Process_request(
            req_state, mem_req_state, &tmp_file_data,
            &tmp_result, PINT_CKSIZE);
        if (ret < 0)
        {
            PINT_Free_request_state(mem_req_state);
            PINT_Free_request_state(req_state);
            return ret;
        }

        /* check if we found data that belongs to this handle */
        if (tmp_result.bytes != 0)
        {
            assert(tmp_result.bytes > 0);

            handle_index_array[(*handle_index_out_count)++] = i;

            gossip_debug(GOSSIP_IO_DEBUG, "%s: "
                         "datafile[%d] might have data (out=%d)\n",
                         __func__, i, *handle_index_out_count);
        }
    }
    PINT_Free_request_state(req_state);
    PINT_Free_request_state(mem_req_state);

    return 0;
}

static int io_datafile_size_comp_fn(
    void *v_p, struct PVFS_server_resp *resp_p, int index)
{
    PINT_client_sm *sm_p = (PINT_client_sm *)v_p;

    if (resp_p->status != 0)
    {
        return resp_p->status;
    }

    assert(resp_p->op == PVFS_SERV_GETATTR);

    gossip_debug(GOSSIP_IO_DEBUG,
                 "  size of datafile %d is %Ld\n",
                 index, Ld(resp_p->u.getattr.attr.u.data.size));

    sm_p->u.io.size_array[index] =
        resp_p->u.getattr.attr.u.data.size;

    return 0;
}

/*
 * Local variables:
 *  mode: c
 *  c-indent-level: 4
 *  c-basic-offset: 4
 * End:
 *
 * vim: ft=c ts=8 sts=4 sw=4 expandtab
 */
