/* 
 * (C) 2003 Clemson University and The University of Chicago 
 *
 * See COPYING in top-level directory.
 */

#include <string.h>
#include <assert.h>

#include "client-state-machine.h"
#include "state-machine-fns.h"
#include "pvfs2-types-debug.h"
#include "pvfs2-debug.h"
#include "job.h"
#include "gossip.h"
#include "str-utils.h"

#include "pinode-helper.h"
#include "pint-dcache.h"
#include "pint-servreq.h"
#include "pint-bucket.h"
#include "pcache.h"
#include "PINT-reqproto-encode.h"

extern job_context_id pint_client_sm_context;

enum {
    IO_NO_DATA = 1,
    IO_DATAFILE_TRANSFERS_COMPLETE = 2
};

/* state function prototypes */
static int io_object_getattr_setup_msgpair(PINT_client_sm *sm_p,
					   job_status_s *js_p);
static int io_object_getattr_failure(PINT_client_sm *sm_p,
				     job_status_s *js_p);
static int io_datafile_setup_msgpairs(PINT_client_sm *sm_p,
				      job_status_s *js_p);
static int io_datafile_post_msgpairs(PINT_client_sm *sm_p,
				     job_status_s *js_p);
static int io_datafile_complete_msgpairs(PINT_client_sm *sm_p,
					 job_status_s *js_p);
static int io_analyze_results(PINT_client_sm *sm_p,
			      job_status_s *js_p);

/* completion function prototypes */
static int io_object_getattr_comp_fn(void *v_p,
				     struct PVFS_server_resp *resp_p,
				     int index);

/* other helper functions */
static int io_find_target_datafiles(PVFS_Request mem_req,
				    PVFS_Request file_req,
				    PVFS_offset file_req_offset, 
				    PVFS_Dist *dist_p,
				    PVFS_handle *input_handle_array,
				    int input_handle_count,
				    PVFS_handle *output_handle_array, 
				    int *handle_count_out_p);

%%

machine pvfs2_client_io_sm(object_getattr_setup_msgpair,
			   object_getattr_xfer_msgpair,
			   object_getattr_failure,
			   datafile_setup_msgpairs,
			   datafile_post_msgpairs,
			   datafile_complete_msgpairs,
			   analyze_results)
{
    state object_getattr_setup_msgpair {
	run io_object_getattr_setup_msgpair;
	default => object_getattr_xfer_msgpair;
    }
    state object_getattr_xfer_msgpair {
	jump pvfs2_client_getattr_pcache_sm;
	success => datafile_setup_msgpairs;
	default => object_getattr_failure;
    }
    state object_getattr_failure {
	run io_object_getattr_failure;
	default => analyze_results;
    }

    state datafile_setup_msgpairs {
	run io_datafile_setup_msgpairs;
	default => datafile_post_msgpairs;
    }
    state datafile_post_msgpairs {
	run io_datafile_post_msgpairs;
	success => datafile_complete_msgpairs;
	default => analyze_results;
    }
    state datafile_complete_msgpairs {
	run io_datafile_complete_msgpairs;
	success => datafile_complete_msgpairs;
	IO_DATAFILE_TRANSFERS_COMPLETE => analyze_results;
	default => datafile_complete_msgpairs;
    }

    state analyze_results {
	run io_analyze_results;
	default => analyze_results;
    }
}

%%

/* PVFS_sys_io()
 */
int PVFS_sys_io(PVFS_pinode_reference pinode_ref,
		PVFS_Request          file_req,
		PVFS_offset           file_req_offset,
		void                 *buffer,
		PVFS_Request          mem_req,
		PVFS_credentials      credentials,
		PVFS_sysresp_io      *resp_p,
		enum PVFS_io_type     io_type)
{
    int ret;
    PINT_client_sm *sm_p;
    PVFS_error error;

    if (resp_p == NULL) {
	gossip_lerr("NULL resp pointer\n");
	return -PVFS_EINVAL;
    }

    if (io_type != PVFS_IO_READ && io_type != PVFS_IO_WRITE) {
	return -PVFS_EINVAL;
    }

    /* allocate and build PINT_client_sm structure */
    sm_p = (PINT_client_sm *) malloc(sizeof(*sm_p));
    if (sm_p == NULL) return -PVFS_ENOMEM;

    memset(sm_p, 0, sizeof(*sm_p));

    sm_p->cred_p = &credentials;
    sm_p->u.io.object_ref    = pinode_ref;
    sm_p->u.io.io_type       = io_type;
    sm_p->u.io.file_req        = file_req;
    sm_p->u.io.file_req_offset = file_req_offset;
    sm_p->u.io.io_resp_p     = resp_p;
    sm_p->u.io.mem_req       = mem_req;
    sm_p->u.io.buffer        = buffer; 

    ret = PINT_client_state_machine_post(sm_p, PVFS_SYS_IO);
    assert(ret == 0);

    while (!sm_p->op_complete && ret == 0) {
	ret = PINT_client_state_machine_test();
    }
    assert(ret == 0);

    error = sm_p->error_code;

    /*
      invalidate pinode entry's size on write (and the meta
      distribution to force a recomputation), if pinode exists
    */
    if (sm_p->pcache_hit && (io_type == PVFS_IO_WRITE))
    {
        /* invalidate the size attribute in the pinode */
        sm_p->pinode->attr.mask &=
            ~(PVFS_ATTR_DATA_SIZE | PVFS_ATTR_META_DIST);

        PINT_pcache_set_valid(sm_p->pinode);
        PINT_pcache_release(sm_p->pinode);
        sm_p->pcache_hit = 0;
    }
    free(sm_p);

    return error;
}

/*******************************************************************/

static int io_object_getattr_setup_msgpair(PINT_client_sm *sm_p,
					   job_status_s *js_p)
{
    uint32_t attrmask;
    int ret = -1;

    gossip_debug(CLIENT_DEBUG,
		 "(%p) io state: io_object_getattr_setup_msgpair\n",
		 sm_p);

    /* we need datafile handles, distribution, and common attribs */
    attrmask = (PVFS_ATTR_META_DFILES |
		PVFS_ATTR_META_DIST |
		PVFS_ATTR_COMMON_ALL);

    PINT_SERVREQ_GETATTR_FILL(sm_p->msgpair.req,
			      *sm_p->cred_p,
			      sm_p->u.io.object_ref.fs_id,
			      sm_p->u.io.object_ref.handle,
			      attrmask);

    /* fill in msgpair structure components */
    sm_p->msgpair.fs_id = sm_p->u.io.object_ref.fs_id;
    sm_p->msgpair.handle = sm_p->u.io.object_ref.handle;
    sm_p->msgpair.comp_fn = io_object_getattr_comp_fn;
    ret = PINT_bucket_map_to_server(&sm_p->msgpair.svr_addr,
	sm_p->msgpair.handle,
	sm_p->msgpair.fs_id);
    if(ret != 0)
    {
	gossip_err("Error: failure mapping to server.\n");
	assert(ret < 0); /* return value range check */
	assert(0); /* TODO: real error handling */
    }


    /* drop through and jump to getattr_pcache state machine */
    return 1;
}

/* io_object_getattr_comp_fn()
 *
 * Called to copy data from getattr response into the io-specific portion of
 * the PINT_client_sm structure, so we can use the data after returning to
 * this state machine.
 *
 * Return value is returned in job status, so it affects the resulting state
 * coming back from the nested state machine.
 *
 * Part of me would like to not have to replicate this code in every instance
 * that we want to grab this data; this particular function was ripped from
 * the getattr state machine and then modified.  However, I'm not sure if
 * there is a good way to generalize this.
 *
 * Returns 0 for metafiles, error for all the other types.
 */
static int io_object_getattr_comp_fn(void *v_p,
				     struct PVFS_server_resp *resp_p,
				     int index)
{
    PVFS_object_attr *r_attr_p;

    /* this is a little kludge to get around some struct definition
     * issues in the headers.  maybe fix later?
     */
    PINT_client_sm *sm_p = (PINT_client_sm *) v_p;

    assert(resp_p->op == PVFS_SERV_GETATTR);

    /* if we get an error, just return immediately, don't try to
     * actually fill anything in.
     */
    if (resp_p->status != 0)
    {
	return resp_p->status;
    }

    r_attr_p = (sm_p->pcache_hit ?
                &sm_p->pinode->attr :
                &resp_p->u.getattr.attr);

    /* sanity checks */
    assert(r_attr_p->mask & PVFS_ATTR_COMMON_TYPE);
    assert(r_attr_p->mask & PVFS_ATTR_META_DIST);
    assert(r_attr_p->u.meta.dist_size > 0);
    assert(r_attr_p->mask & PVFS_ATTR_META_DFILES);
    assert(r_attr_p->u.meta.dfile_count > 0);

    if (r_attr_p->objtype != PVFS_TYPE_METAFILE)
    {
	/* invalid type - we only do I/O on files */
	return -PVFS_EINVAL;
    }

    if (!sm_p->pcache_hit)
    {
        int release_required = 1;
        PINT_pinode *pinode =
            PINT_pcache_lookup(sm_p->u.io.object_ref);
        if (!pinode)
        {
            pinode = PINT_pcache_pinode_alloc();
            assert(pinode);
            release_required = 0;
        }
        pinode->refn = sm_p->u.io.object_ref;

        PINT_pcache_object_attr_deep_copy(
            &pinode->attr, r_attr_p);

        PINT_pcache_set_valid(pinode);

        if (release_required)
        {
            PINT_pcache_release(pinode);
        }

        /*
          FIXME: Replace with PVFS_Dist_copy, which
          causes problems when used here
        */

        /* here we make a copy of the distribution information. */
        gossip_debug(CLIENT_DEBUG,
                     "io_object_getattr_comp_fn: copying %d bytes "
                     "of dist.\n", r_attr_p->u.meta.dist_size);

        sm_p->u.io.dist_p = (PVFS_Dist *)
            malloc(r_attr_p->u.meta.dist_size);
        if (sm_p->u.io.dist_p == NULL)
        {
            assert(0);
            return -PVFS_ENOMEM;
        }

        /* this encodes the previously decoded distribution into
         * our new space.
         */
        PINT_Dist_encode(sm_p->u.io.dist_p, r_attr_p->u.meta.dist);

        /* this does an in-place decoding of the distribution.  now
         * we have a decoded version where we want it.
         *
         * NOTE: we need to free this later.
         */
        PINT_Dist_decode(sm_p->u.io.dist_p, NULL);

        assert(sm_p->u.io.dist_p);

        /* save the size too */
        sm_p->u.io.dist_size = r_attr_p->u.meta.dist_size;

        gossip_debug(CLIENT_DEBUG,
                     "io_object_getattr_comp_fn: %d datafiles.\n",
                     r_attr_p->u.meta.dfile_count);
		
        /* save the datafile handles prior to freeing up the
         * buffers we used for messages.  we could keep them around
         * i suppose, but we're not going to do that for now.  later
         * it is likely that this stuff will be stuck in the pcache
         * anyway, so we'll be able to just reference it from there.
         */
        sm_p->u.io.datafile_handles = (PVFS_handle *)
            malloc(r_attr_p->u.meta.dfile_count * sizeof(PVFS_handle));
        if (sm_p->u.io.datafile_handles == NULL)
        {
            assert(0);
        }
        sm_p->u.io.datafile_count = r_attr_p->u.meta.dfile_count;
        memcpy(sm_p->u.io.datafile_handles,
               r_attr_p->u.meta.dfile_array,
               r_attr_p->u.meta.dfile_count * sizeof(PVFS_handle));
    }
    else
    {
        /* fast path; no copying needed */
        assert(sm_p->pinode);

        sm_p->u.io.dist_p = sm_p->pinode->attr.u.meta.dist;
        sm_p->u.io.dist_size = sm_p->pinode->attr.u.meta.dist_size;
        sm_p->u.io.datafile_handles =
            sm_p->pinode->attr.u.meta.dfile_array;
        sm_p->u.io.datafile_count =
            sm_p->pinode->attr.u.meta.dfile_count;        
    }
    return 0;
} /* io_object_getattr_comp_fn */

/* io_datafile_setup_io()
 *
 * Sets up msgpairs to send I/O requests to servers holding datafiles
 * that (might) have data for us (unless we hit EOF).
 *
 * This function swaps the original datafile_handles array for a new
 * array that just has datafiles that we think will have our data.  It
 * updates datafile_count appropriately as well.
 *
 * We use sm_p->msgarray for this purpose.
 *
 * NOTE: we could combine this with the post_msgpairs state, but this one
 *       has gotten pretty big already, so let's not.
 *
 * TODO: MERGE A BUNCH OF THESE MALLOCS INTO LARGER SINGLE MALLOC TO SAVE
 *       TIME AND COMPLEXITY IN ERROR CASE.
 */
static int io_datafile_setup_msgpairs(PINT_client_sm *sm_p,
				      job_status_s *js_p)
{
    int i, ret;
    int target_datafile_count;
    PVFS_handle *target_datafile_array;

    struct PINT_client_io_sm *iosm_p = &sm_p->u.io; /* convenience */

    gossip_debug(CLIENT_DEBUG,
		 "(%p) io state: io_datafile_setup_io\n",
		 sm_p);

    /* NOTE: i have no idea what this function does.  presumably it has
     * some important side-effects, since it doesn't have an output
     * parameter.
     *
     * TODO: make the Dist interface make sense.
     */
    ret = PINT_Dist_lookup(iosm_p->dist_p);
    if (ret != 0) {
	assert(0);
    }

    target_datafile_array =
	(PVFS_handle *) malloc(iosm_p->datafile_count * sizeof(PVFS_handle));
    if (target_datafile_array == NULL) {
	assert(0);
    }

    ret = io_find_target_datafiles(iosm_p->mem_req,
				   iosm_p->file_req,
				   iosm_p->file_req_offset,
				   iosm_p->dist_p,
				   iosm_p->datafile_handles,
				   iosm_p->datafile_count,
				   target_datafile_array,
				   &target_datafile_count);
    if (ret != 0) {
	assert(0);
    }

    if (target_datafile_count == 0) {
	/* no data?  ok... */
	/* TODO: catch this earlier or something. */
	js_p->error_code = IO_NO_DATA;
	return 1;
    }

    gossip_debug(CLIENT_DEBUG,
		 "  datafile_setup_msgpairs: %d datafiles might have data\n",
		 target_datafile_count);

    /* set up space for msgarrray */
    sm_p->msgarray = (PINT_client_sm_msgpair_state *)
	malloc(target_datafile_count * sizeof(PINT_client_sm_msgpair_state));
    if (sm_p->msgarray == 0) {
	assert(0);
    }
    sm_p->msgarray_count = target_datafile_count;

    /* set up space for flows */
    iosm_p->flow_comp_ct = 0;

    /* flow descriptor array */
    iosm_p->flow_array = (flow_descriptor *)
	malloc(target_datafile_count * sizeof(flow_descriptor));
    if (iosm_p->flow_array == NULL) {
	assert(0);
    }
    /* set each flow descriptor to initial values */
    for(i=0; i<target_datafile_count; i++)
    {
	PINT_flow_reset(&iosm_p->flow_array[i]);
    }

    /* flow status array */
    iosm_p->flow_status_array = (job_status_s *)
	malloc(target_datafile_count * sizeof(job_status_s));
    if (iosm_p->flow_status_array == NULL) {
	assert(0);
    }

    /* session tag array */
    iosm_p->session_tag_array = (PVFS_msg_tag_t *)
	malloc(target_datafile_count * sizeof(PVFS_msg_tag_t));
    if (iosm_p->session_tag_array == NULL) {
	assert(0);
    }

    /* set up space for final ack if we're writing */
    if (iosm_p->io_type == PVFS_IO_WRITE) {
	/* write acknowledgement array */
	iosm_p->ack_comp_ct = 0;
	iosm_p->ackarray = (PINT_client_sm_recv_state *)
	    malloc(target_datafile_count * sizeof(PINT_client_sm_recv_state));
	if (iosm_p->ackarray == NULL) {
	    assert(0);
	}
	memset(iosm_p->ackarray, 0,
	    target_datafile_count*sizeof(PINT_client_sm_recv_state));
    }

    /* fill in all the I/O requests */
    for (i=0; i < target_datafile_count; i++) {
	int orig_index;
	PINT_client_sm_msgpair_state *msg_p = &sm_p->msgarray[i];

	gossip_debug(CLIENT_DEBUG,
		     "  sending I/O request for 0x%08Lx\n",
		     target_datafile_array[i]);

	/* find the original index for the handle */
	for (orig_index = 0; orig_index < iosm_p->datafile_count; orig_index++)
	{
	    if (target_datafile_array[i] == iosm_p->datafile_handles[orig_index])
		break;
	}
	assert(orig_index < iosm_p->datafile_count); /* sanity check */

	/* fill in I/O request */
	PINT_SERVREQ_IO_FILL(msg_p->req,
			     *sm_p->cred_p,
			     iosm_p->object_ref.fs_id,
			     target_datafile_array[i],
			     iosm_p->io_type,
			     FLOWPROTO_ANY,
			     orig_index,
			     target_datafile_count,
			     iosm_p->dist_p,
			     iosm_p->file_req,
			     iosm_p->file_req_offset,
			     PINT_REQUEST_TOTAL_BYTES(iosm_p->mem_req));

	/* fill in msgpair structure components */
	msg_p->fs_id   = iosm_p->object_ref.fs_id;
	msg_p->handle  = iosm_p->datafile_handles[i];
	msg_p->comp_fn = NULL;
    }

    /*
     * Swap the new list in for the old one, freeing the old list.
     */
    if (!sm_p->pcache_hit)
    {
        free(iosm_p->datafile_handles);
    }
    iosm_p->datafile_handles = target_datafile_array;
    iosm_p->datafile_count   = target_datafile_count;
    
    return 1;
}

/* io_datafile_post_msgpairs()
 *
 * This is basically a copy of msgpairarray.c:msgpairarray_post().
 * We need to handle the rest of the process somewhat differently though,
 * so we're going to have our own versions here.
 *
 * We use the msgarray to keep up with the initial send/recv pairs.
 *
 */
static int io_datafile_post_msgpairs(PINT_client_sm *sm_p,
				     job_status_s *js_p)
{
    int ret, i;

    gossip_debug(CLIENT_DEBUG,
		 "io_datafile_post_msgpairs state: post (%d message(s))\n",
		 sm_p->msgarray_count);

    assert(sm_p->msgarray_count > 0); /* sanity check */

    /* we don't know what this is set to prior to this function,
     * so we reset it here.
     */
    js_p->error_code = 0;

    /* set number of operations that must complete.
     *
     * NOTE: we're using the comp_ct in the first msgarray
     * entry to keep up with the count for the entire array.
     */
    sm_p->msgarray[0].comp_ct = 2 * sm_p->msgarray_count;

    /* run through array of msgpairarray to kick off */
    for (i=0; i < sm_p->msgarray_count; i++) {
	PVFS_msg_tag_t session_tag;
	PINT_client_sm_msgpair_state *msg_p = &sm_p->msgarray[i];

	/* determine server address from fs_id/handle pair.
	 * this is needed prior to encoding.
	 */
	ret = PINT_bucket_map_to_server(&msg_p->svr_addr,
					msg_p->handle,
					msg_p->fs_id);
	if (ret != 0) {
	    gossip_lerr("bucket map to server failed; probably invalid svr_addr\n");
	    assert(ret < 0); /* return value range check */
	    assert(0); /* placeholder until we have real error handling */
	}

	/* encode request. fills in encoded_req.
	 */
	ret = PINT_encode(&msg_p->req,
			  PINT_ENCODE_REQ,
			  &msg_p->encoded_req,
			  msg_p->svr_addr,
			  PINT_CLIENT_ENC_TYPE);
	if (ret != 0) {
	    gossip_lerr("pint_encode failed\n");
	    assert(ret < 0); /* return value range check */
	    assert(0); /* placeholder until we have real error handling */
	}

	/* calculate maximum response message size and allocate space.
	 * fills in max_resp_sz, encoded_resp_p
	 */
	msg_p->max_resp_sz = PINT_encode_calc_max_size(PINT_ENCODE_RESP,
						       msg_p->req.op,
						       PINT_CLIENT_ENC_TYPE);
	msg_p->encoded_resp_p = BMI_memalloc(msg_p->svr_addr,
					     msg_p->max_resp_sz,
					     BMI_RECV);
	if (msg_p->encoded_resp_p == NULL) {
	    assert(0);
	}

	/* get session tag to associate with send and receive.
	 * session tag is kept in io part of client state machine
	 * structure for use in the flow and option final ack.
	 */
	session_tag = get_next_session_tag();
	sm_p->u.io.session_tag_array[i] = session_tag;

	/* post receive of response; job_id stored in recv_id */
	ret = job_bmi_recv(msg_p->svr_addr,
			   msg_p->encoded_resp_p,
			   msg_p->max_resp_sz,
			   session_tag,
			   BMI_PRE_ALLOC,
			   sm_p,
			   i,
			   &msg_p->recv_status,
			   &msg_p->recv_id,
			   pint_client_sm_context);
	if (ret < 0) {
	    gossip_lerr("post of receive failed\n");
	    assert(0);
	}
	else if (ret == 1) {
	    /* it shouldn't be possible for the receive to complete before
	     * we send the request.
	     */
	    assert(0); /* sanity check */
	}
	assert(ret == 0); /* return value range check */

	/* post send of request; job_id stored in send_id */
	ret = job_bmi_send_list(msg_p->encoded_req.dest,
				msg_p->encoded_req.buffer_list,
				msg_p->encoded_req.size_list,
				msg_p->encoded_req.list_count,
				msg_p->encoded_req.total_size,
				session_tag,
				msg_p->encoded_req.buffer_type,
				1,
				sm_p,
				(sm_p->msgarray_count + i),
				&msg_p->send_status,
				&msg_p->send_id,
				pint_client_sm_context);
	if (ret < 0) {
	    gossip_lerr("post of send failed\n");
	    assert(0);
	}
	else if (ret == 1) {
	    /* send completed immediately; decrement the completion counter */
	    gossip_debug(CLIENT_DEBUG,
			 "  io_datafile_post_msgpairs: send completed immediately.\n");

	    /* 0 is the valid "completed job id" value, according to Phil */
	    msg_p->send_id = 0;

	    /* TODO: CHECK THE STATUS!!! */
	    if (msg_p->send_status.error_code != 0) {
		PVFS_perror_gossip("sys-io.sm: msg_p->send_status.error_code", msg_p->send_status.error_code);
		assert(0);
	    }

	    /* decrement our count, since send is already done.
	     *
	     * recall we're using the comp_ct in the first array
	     * element to keep up with our count for the entire
	     * array.
	     */
	    sm_p->msgarray[0].comp_ct--;
	}
	assert(ret == 0 || ret == 1); /* return value range check */
    }

    return 0;
}

/* io_datafile_complete_msgpairs()
 *
 * This started off as a copy of msgpairarray.c:msgpairarray_complete(),
 * but we need to post flow operations as the bmi operations complete,
 * so we have some additional work to do.
 *
 * TODO: THIS FUNCTION IS TOTAL MADNESS AT THIS POINT; BREAK IT UP OR
 *       SOMETHING FOR GOD'S SAKE!!!
 */
static int io_datafile_complete_msgpairs(PINT_client_sm *sm_p,
					 job_status_s *js_p)
{
    int i, ret;
    int matched_msgpair = 0, matched_flow = 0, matched_ack = 0, recv_match = -1;
    job_id_t tmp_id;

    gossip_debug(CLIENT_DEBUG, "io state: datafile_complete\n");

    /* loads of sanity checks */
    assert(sm_p->msgarray_count == sm_p->u.io.datafile_count);
    assert(sm_p->msgarray[0].comp_ct >= 0);
    assert(sm_p->u.io.flow_comp_ct >= 0);
    assert(sm_p->u.io.ack_comp_ct >= 0);
    assert(js_p->status_user_tag < (sm_p->msgarray_count*4));

    /* if there are outstanding msgpairs to complete, try to match
     * whatever completed with something in the msgpair array
     */
    if (sm_p->msgarray[0].comp_ct > 0) {
	if(js_p->status_user_tag < sm_p->msgarray_count)
	{
	    /* first N user tags are for receives */
	    PINT_client_sm_msgpair_state *msg_p = 
		&sm_p->msgarray[js_p->status_user_tag];
	    matched_msgpair = 1;
	    recv_match = js_p->status_user_tag;
	    msg_p->recv_id     = 0;
	    msg_p->recv_status = *js_p;
	    assert(msg_p->recv_status.actual_size <= msg_p->max_resp_sz);
	    assert(msg_p->recv_status.error_code <= 0); /* range check */
	}
	else if(js_p->status_user_tag < (sm_p->msgarray_count*2))
	{
	    /* second N user tags are for sends */
	    PINT_client_sm_msgpair_state *msg_p = 
		&sm_p->msgarray[js_p->status_user_tag - sm_p->msgarray_count];
	    matched_msgpair = 1;
	    msg_p->send_id     = 0;
	    msg_p->send_status = *js_p;
	    assert(msg_p->send_status.error_code <= 0); /* range check */
	}

	if (matched_msgpair) {
	    /* decrement comp_ct until all operations have completed.
	     *
	     * recall we're using the comp_ct in the first array element...
	     */
	    sm_p->msgarray[0].comp_ct--;

	    if (sm_p->msgarray[0].comp_ct == 0) {
		gossip_debug(CLIENT_DEBUG,
			     "  all msgpairs complete.\n");
	    }
	}
    }

    if (matched_msgpair && recv_match == -1) {
	gossip_debug(CLIENT_DEBUG, "  matched send to %d; continuing.\n", i);

	/* matched a completed send; just return */
	return 0;
    }

    /* if we matched a receive, then we need to decode the receive,
     * post the appropriate flow, and possibly post the receive of an ack
     * BMI message (if a write operation).
     *
     * we use the same index into the various arrays in the io part of the
     * state machine state that we used for this receive job (recv_match).
     */
    if (recv_match != -1) {
	PINT_client_sm_msgpair_state *msg_p = &sm_p->msgarray[recv_match];
	struct PINT_decoded_msg decoded_resp; /* data about decoded resp */
	struct PVFS_server_resp *resp_p; /* response structure (decoded) */

	gossip_debug(CLIENT_DEBUG, "  matched response from %d.\n", i);

	ret = PINT_serv_decode_resp(msg_p->encoded_resp_p,
				    &decoded_resp,
				    &msg_p->svr_addr,
				    msg_p->recv_status.actual_size,
				    &resp_p);
	if (ret != 0) {
	    gossip_lerr("error: io_datafile_complete_msgpairs: decode error\n");
	    assert(ret < 0); /* return value range check */
	    assert(0); /* placeholder */
	}

	assert(resp_p->status <= 0); /* parameter range checking */
	msg_p->op_status = resp_p->status; /* save status value */

	/* note: we saved the recv_status up above (before decoding) */

	if (msg_p->recv_status.error_code != 0 || msg_p->op_status != 0)
	{
	    gossip_debug(CLIENT_DEBUG,
			 "  error related to response from %d; not submitting flow.\n",
			 recv_match);
	    ret = PINT_serv_free_msgpair_resources(&msg_p->encoded_req,
						   msg_p->encoded_resp_p,
						   &decoded_resp,
						   &msg_p->svr_addr,
						   msg_p->max_resp_sz);
	    if (ret != 0) {
		assert(ret < 0); /* return value range check */
		assert(0); /* placeholder */
	    }

	    return 0;
	}
	else
	{
	    flow_descriptor *fl_p;
	    struct PINT_client_io_sm *iosm_p = &sm_p->u.io;

	    gossip_debug(CLIENT_DEBUG, "  building flow for %d.\n", recv_match);

	    /* find matching flow descriptor */
	    fl_p = &sm_p->u.io.flow_array[recv_match];

	    /* fill in file_data structure */
	    /* TODO: MACRO TO FILL THIS IN? */
	    fl_p->file_data.fsize     = resp_p->u.io.bstream_size;
	    fl_p->file_data.dist      = iosm_p->dist_p;
	    fl_p->file_data.iod_num   = recv_match;
	    fl_p->file_data.iod_count = iosm_p->datafile_count;

	    /* this is the file datatype */
	    fl_p->file_req        = iosm_p->file_req;
	    fl_p->file_req_offset = iosm_p->file_req_offset;
	    /* memory datatype */
	    fl_p->mem_req         = iosm_p->mem_req;

	    gossip_debug(CLIENT_DEBUG,
			 "    bstream_size = %Ld, datafile_nr = %d, datafile_ct = %d, file_req_off = %Ld\n",
			 fl_p->file_data.fsize,
			 fl_p->file_data.iod_num,
			 fl_p->file_data.iod_count,
			 fl_p->file_req_offset);

	    /* session tag, same as the one used in the send/recv */
	    fl_p->tag = iosm_p->session_tag_array[recv_match];

	    /* this user_ptr should be left alone */
	    fl_p->user_ptr = NULL;

	    /* TODO: SAVE THE FLOWPROTO SOMEWHERE? */
	    fl_p->type = FLOWPROTO_ANY;

	    /* done with msgpair resources; free them now */
	    ret = PINT_serv_free_msgpair_resources(&msg_p->encoded_req,
						   msg_p->encoded_resp_p,
						   &decoded_resp,
						   &msg_p->svr_addr,
						   msg_p->max_resp_sz);
	    if (ret != 0) {
		assert(ret < 0); /* return value range check */
		assert(0); /* placeholder */
	    }

	    /* NOTE: NO USING RESP_P AFTER WE FREE THE RESOURCES */
	    resp_p = NULL;

	    if (iosm_p->io_type == PVFS_IO_READ) {
		/* Set up read-specific values
		 * - don't keep going past EOF
		 * - data is coming from BMI and going into memory
		 * - data is coming from the same server we got the recv from
		 */
		fl_p->file_data.extend_flag = 0;
		fl_p->src.endpoint_id   = BMI_ENDPOINT;
		fl_p->src.u.bmi.address = msg_p->svr_addr;
		fl_p->dest.endpoint_id  = MEM_ENDPOINT;
		fl_p->dest.u.mem.buffer = iosm_p->buffer;
	    }
	    else {
		assert (iosm_p->io_type == PVFS_IO_WRITE); /* sanity check */

		/* do allow writes to extend the file */
		fl_p->file_data.extend_flag = 1;
		fl_p->src.endpoint_id    = MEM_ENDPOINT;
		fl_p->src.u.mem.buffer   = iosm_p->buffer;
		fl_p->dest.endpoint_id   = BMI_ENDPOINT;
		fl_p->dest.u.bmi.address = msg_p->svr_addr;

		gossip_debug(CLIENT_DEBUG,
			     "  preposting write ack for %d.\n",
			     recv_match);

		/* go ahead and prepost the ack receive now */
		iosm_p->ackarray[recv_match].max_resp_sz =
		    PINT_encode_calc_max_size(PINT_ENCODE_RESP,
					      PVFS_SERV_WRITE_COMPLETION,
					      PINT_CLIENT_ENC_TYPE);
		iosm_p->ackarray[recv_match].encoded_resp_p =
		    BMI_memalloc(msg_p->svr_addr,
				 iosm_p->ackarray[recv_match].max_resp_sz,
				 BMI_RECV);
		if (iosm_p->ackarray[recv_match].encoded_resp_p == NULL) {
		    assert(0);
		}
		ret = job_bmi_recv(msg_p->svr_addr,
				   iosm_p->ackarray[recv_match].encoded_resp_p,
				   iosm_p->ackarray[recv_match].max_resp_sz,
				   iosm_p->session_tag_array[recv_match],
				   BMI_PRE_ALLOC,
				   sm_p,
				   (sm_p->msgarray_count*3 + recv_match),
				   &iosm_p->ackarray[recv_match].recv_status,
				   &iosm_p->ackarray[recv_match].recv_id,
				   pint_client_sm_context);
		if (ret < 0) {
		    gossip_lerr("post of write ack failed\n");
		    assert(0);
		}
		else if (ret == 1) {
		    /* should never happen -- cannot get response before flow? */
		    assert(0);
		}
		assert(ret == 0);
		
		iosm_p->ack_comp_ct++;
	    }

	    /* post the flow:
	     * - pass sm_p in as user pointer, so we get called again
	     * - have status stored in flow status array, if we complete immed.
	     * - save id in flow id array
	     * - use recv_match as index for all this
	     */
	    ret = job_flow(fl_p,
			   sm_p, /* user pointer */
			   (sm_p->msgarray_count*2 + recv_match),
			   &iosm_p->flow_status_array[recv_match],
			   &tmp_id,
			   pint_client_sm_context);
	    if (ret < 0) {
		gossip_lerr("post of flow failed.\n");
		assert(0);
	    }
	    else if (ret == 1) {
		/* flow completed immediately; i guess this is possible... */
		gossip_debug(CLIENT_DEBUG,
			     "  flow for %d completed immediately!\n",
			     recv_match);
		assert(iosm_p->flow_status_array[recv_match].error_code == 0);
	    }
	    else {
		assert(ret == 0); /* return value range check */

		gossip_debug(CLIENT_DEBUG,
			     "  posted flow for %d.\n",
			     recv_match);

		/* record that we have a flow that still needs to complete */
		iosm_p->flow_comp_ct++;
	    }
	}
	return 0;
    }

    /* sanity check to ensure we caught all exits above */
    assert(matched_msgpair == 0);

    /* if we didn't match to a message pair, then we completed either
     * a flow or a write ack
     */

    gossip_debug(CLIENT_DEBUG, "  trying to match to a flow or ack.\n");

    if(js_p->status_user_tag >= (sm_p->msgarray_count*2) &&
	js_p->status_user_tag < (sm_p->msgarray_count*3))
    {
	/* the third set of N user tags correspond to flows */
	matched_flow = 1;
	sm_p->u.io.flow_status_array[js_p->status_user_tag-(sm_p->msgarray_count*2)] 
	    = *js_p;
	sm_p->u.io.flow_comp_ct--;
	gossip_debug(CLIENT_DEBUG,
		     "  matched completed flow for %d\n",
		     (js_p->status_user_tag-(sm_p->msgarray_count*2)));

	assert(sm_p->u.io.flow_comp_ct >= 0); /* sanity check */
    }
    else if(js_p->status_user_tag >= (sm_p->msgarray_count*3))
    {
	/* the fourth set of N user tags correspond to final acks */
	matched_ack = 1;
	i = js_p->status_user_tag - (sm_p->msgarray_count*3);
	sm_p->u.io.ackarray[i].recv_id      = 0;
	sm_p->u.io.ackarray[i].recv_status = *js_p;
	assert(sm_p->u.io.ackarray[i].recv_status.actual_size 
	    <= sm_p->u.io.ackarray[i].max_resp_sz);
	sm_p->u.io.ack_comp_ct--;
	gossip_debug(CLIENT_DEBUG,
		     "  matched completed ack for %d\n",
		     i);

	assert(sm_p->u.io.ack_comp_ct >= 0);
    }
    
    if (sm_p->u.io.flow_comp_ct == 0 &&
	sm_p->u.io.ack_comp_ct == 0 &&
	sm_p->msgarray[0].comp_ct == 0)
    {
	gossip_debug(CLIENT_DEBUG, "  all msgpairs, flows, ack completed.\n");

	/* quit returning to this state! */
	js_p->error_code = IO_DATAFILE_TRANSFERS_COMPLETE;
	return 1;
    }
    else {
	/* still something left to transfer */
	return 0;
    }
}

static int io_object_getattr_failure(PINT_client_sm *sm_p,
				     job_status_s *js_p)
{
    gossip_debug(CLIENT_DEBUG,
		 "(%p) io state: io_object_getattr_failure\n",
		 sm_p);
    assert(0);

    return 1;
}

/* io_analyze_results()
 */
static int io_analyze_results(PINT_client_sm *sm_p,
			      job_status_s *js_p)
{
    int i, ret, error = 0, error_count = 0;
    PVFS_size total_size = 0;

    gossip_debug(CLIENT_DEBUG,
		 "(%p) io state: analyze_results\n",
		 sm_p);

    if (js_p->error_code != IO_DATAFILE_TRANSFERS_COMPLETE) {
	/* some sort of error occurred early on */
    }
    else {
	/* it's possible that some error occurred still.  look through
	 * all the messages, reporting errors, saving the first one to
	 * return, and adding up the size of the transfer (just in case
	 * things actually completed).
	 *
	 */
	for (i=0; i < sm_p->msgarray_count; i++) {
	    PINT_client_sm_msgpair_state *msg_p = &sm_p->msgarray[i];
	    struct PINT_client_io_sm *iosm_p = &sm_p->u.io;

	    if (msg_p->send_status.error_code != 0)
	    {
		gossip_debug(CLIENT_DEBUG,
			     "  error (%d) in msgpair send for %d.\n",
			     msg_p->send_status.error_code,
			     i);
		error_count++;
		if (error == 0) error = msg_p->send_status.error_code;
	    }
	    else if (msg_p->recv_status.error_code != 0)
	    {
		gossip_debug(CLIENT_DEBUG,
			     "  error (%d) in msgpair recv for %d.\n",
			     msg_p->recv_status.error_code,
			     i);
		error_count++;
		if (error == 0) error = msg_p->recv_status.error_code;
	    }
	    /* NOTE: this is a little bit of a hack to determine if we have
	     * actually used this flow descriptor; we don't really care
	     * what the file_req is
	     */
	    else if (iosm_p->flow_array[i].file_req != NULL)
	    {
		if (iosm_p->flow_status_array[i].error_code != 0)
		{
		    gossip_debug(CLIENT_DEBUG,
				 "  error (%d) in flow for %d.\n",
				 iosm_p->flow_status_array[i].error_code,
				 i);
		    error_count++;
		    if (error == 0) {
			error = iosm_p->flow_status_array[i].error_code;
		    }
		}
		else if (iosm_p->io_type == PVFS_IO_READ)
		{
		    /* size for reads is reported in the flow;
		     * size for writes is reported in the final ack.
		     */
		    gossip_debug(CLIENT_DEBUG,
				 "  %Ld bytes read from %d.\n",
				 iosm_p->flow_array[i].total_transfered,
				 i);
		    total_size += iosm_p->flow_array[i].total_transfered;
		}
		else if (iosm_p->io_type == PVFS_IO_WRITE) {
		    if (iosm_p->ackarray[i].recv_status.error_code != 0)
		    {
			gossip_debug(CLIENT_DEBUG,
				     "  error (%d) in write ack for %d.\n",
				     iosm_p->ackarray[i].recv_status.error_code,
				     i);
			error_count++;
			if (error == 0) {
			    error = iosm_p->ackarray[i].recv_status.error_code;
			}
		    }
		    else
		    {
			struct PINT_decoded_msg decoded_resp;
			struct PVFS_server_resp *resp_p;
			
			/* we need to decode the write ack so we can see how
			 * much data we were actually able to write.
			 *
			 * note: we just use the svr_addr from the msgpair; it's
			 *       the same address.
			 */
			ret = PINT_serv_decode_resp(iosm_p->ackarray[i].encoded_resp_p,
						    &decoded_resp,
						    &msg_p->svr_addr,
						    iosm_p->ackarray[i].recv_status.actual_size,
						    &resp_p);
			if (ret != 0) {
			    assert(0);
			}
			gossip_debug(CLIENT_DEBUG,
				     "  %Ld bytes written to %d.\n",
				     resp_p->u.write_completion.total_completed,
				     i);
			total_size += resp_p->u.write_completion.total_completed;

			/* done with this; free decode resources */
			PINT_decode_release(&decoded_resp, PINT_DECODE_RESP);
		    }
		}

		/* free resources associated with this flow array element:
		 * - write ack, if it was a write (we decoded it if we needed
		 *   to above, and got rid of that already)
		 */
		PINT_flow_reset(&sm_p->u.io.flow_array[i]); /* pedantic */
		if (iosm_p->io_type == PVFS_IO_WRITE) {
		    BMI_memfree(msg_p->svr_addr,
				iosm_p->ackarray[i].encoded_resp_p,
				iosm_p->ackarray[i].max_resp_sz,
				BMI_RECV);
		}

	    }
	}
    }

    /* free the rest of the resources:
     * - msgpairarray
     * - datafile handle array (if allocated)
     * - flow descriptor pointer array
     * - file data array
     * - flow id array
     * - flow status array
     * - session tag array
     * - ackarray
     */
    free(sm_p->msgarray);

    if (!sm_p->pcache_hit)
    {
        free(sm_p->u.io.datafile_handles);
    }
    free(sm_p->u.io.flow_array);
    free(sm_p->u.io.flow_status_array);
    free(sm_p->u.io.session_tag_array);
    free(sm_p->u.io.ackarray);

    /* return size, error, and set operation as complete */
    sm_p->u.io.io_resp_p->total_completed = total_size;
    sm_p->error_code = error;
    sm_p->op_complete = 1;

    return 0;
}

/********************************************************************/

/* io_find_target_datafiles()
 *
 * determines what subset of the datafiles actually contain data that we
 * are interested in for this request
 *
 * returns 0 on success, -pvfs_error on failure
 *
 * TODO: make this step more efficient
 *
 * NOTE: this was ripped out of io.c and then modified.
 */
static int io_find_target_datafiles(PVFS_Request mem_req,
				    PVFS_Request file_req,
				    PVFS_offset file_req_offset, 
				    PVFS_Dist *dist_p,
				    PVFS_handle *input_handle_array,
				    int input_handle_count,
				    PVFS_handle *output_handle_array, 
				    int *handle_count_out_p)
{
    int i, ret;
    struct PINT_Request_state *req_state = NULL;
    struct PINT_Request_state *mem_req_state = NULL;
    PINT_Request_file_data tmp_file_data;
    PINT_Request_result tmp_result;

    /* initialize output handle count */
    *handle_count_out_p = 0;

    req_state = PINT_New_request_state(file_req);
    if(!req_state)
    {
	return -PVFS_ENOMEM;
    }
    mem_req_state = PINT_New_request_state(mem_req);
    if(!mem_req_state)
    {
	PINT_Free_request_state(req_state);
	return -PVFS_ENOMEM;
    }

    tmp_file_data.dist = dist_p;
    tmp_file_data.iod_count = input_handle_count;
    tmp_file_data.extend_flag = 1;

    for (i=0; i < input_handle_count; i++)
    {
	/* NOTE: we don't have to give an accurate file size here,
	 * as long as we set the extend flag to tell the I/O req
	 * processor to continue past eof if needed
	 */
	tmp_file_data.fsize = 0;  
	tmp_file_data.iod_num = i;

	PINT_REQUEST_STATE_RESET(req_state);
	PINT_REQUEST_STATE_RESET(mem_req_state);

	/* if a file datatype offset was specified, go ahead and skip ahead 
	 * before calculating
	 */
	if (file_req_offset)
	    PINT_REQUEST_STATE_SET_TARGET(req_state, file_req_offset);

	PINT_REQUEST_STATE_SET_FINAL(req_state, 
	    file_req_offset+PINT_REQUEST_TOTAL_BYTES(mem_req));

	memset(&tmp_result, 0, sizeof(PINT_Request_result));
	tmp_result.bytemax = 1;
	tmp_result.segmax = 1;

	/* PINT_Process_request() returns number of bytes processed */
	ret = PINT_Process_request(req_state,
				   mem_req_state,
				   &tmp_file_data,
				   &tmp_result,
				   PINT_CKSIZE);
	if (ret < 0)
	{
	    PINT_Free_request_state(mem_req_state);
	    PINT_Free_request_state(req_state);
	    return ret;
	}

	/* did we find that any data belongs to this handle? */
	if (tmp_result.bytes != 0)
	{
	    assert(tmp_result.bytes > 0); /* parameter range checking */

	    gossip_debug(CLIENT_DEBUG,
			 "io_find_target_dfiles: datafile %d might have some data.\n",
			 i);

	    output_handle_array[*handle_count_out_p] = input_handle_array[i]; 
	    (*handle_count_out_p)++;
	}
    }
    PINT_Free_request_state(req_state);
    PINT_Free_request_state(mem_req_state);

    return 0;
} /* io_find_target_datafiles */


/*
 * Local variables:
 *  mode: c
 *  c-indent-level: 4
 *  c-basic-offset: 4
 * End:
 *
 * vim: ft=c ts=8 sts=4 sw=4 noexpandtab
 */
