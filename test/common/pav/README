Readme for pav-0.1:

Idea:

This package is meant to help with the setup and shutdown of PVFS
volumes.  It was written with the intent of making the testing of such
volumes simple, since testing PVFS volumes often times involves the
time consuming process of making changes to PVFS server code, pushing
the compiled binaries out to server nodes, setting up PVFS server
config files, and finally starting the iod and mgr processes.  The pav
package is also useful for users who would like to set up PVFS volumes
on systems which they only have user accounts on.  Such a user could
install the pav package in their home directory and use it inside of a
PBS script to create a useable PVFS volume that only persists for the
duration of their job.  Sysadmins like the author also use the package
to push out new versions of PVFS code onto their cluster wide PVFS
resource.


Using pav:

To use the system (after installation), one must follow these steps:

1.) create a list of machine 'resources'.  This is simple a file
containing a list of machines, one per line.  Exactly the same as a
MPICH machines file.

2.) create a pav configfile (we will call the file pav.conf but it can
be called anything).  The configfile has key=val syntax and can
contain comments (line beginning with '#').  Any value can use the
value of a previously declared key by using a $KEY syntax (see STORAGE
below).  There is a sample configfile with the distribution called
configfile.sample.

The following keys can be defined:

NODEFILE="</path/to/machinefile>"
IONCOUNT=<number of 'io' nodes for the system>
METACOUNT=<number of 'meta' nodes for the system>
UNIQUEMETA=<1 = dedicated metadata server 
		0 = metadata server runs on data server >
PROTOCOL=<'tcp' (TCP/IP), 'gm' (Myrinet), 'ib' (infinband) >

WORKINGDIR="</path/to/pvfs_datadir>" This is where all pvfs data will
be written to on the remote 'io' nodes.  Usually use a large, fast
volume for this directory *NOTE: you should always use a unique, empty
directory for this option.  A pav_stop will remove this entire
directory tree.  The same is true for STORAGE below, we usually put
STORAGE as a subdirectory of WORKINGDIR.

PVFSPORT=<port number to use>
STORAGE="</path/to/storage_adir>" Usually set to $WORKINGDIR/data
SERVERLOG="</path/to/logdir>" Usually set to $WORKINGDIR/log

MOUNTPOINT="</path/to/pvfs_virtual_mountpoint>" 
ENCODING="{direct|le_bfield|xdr}" Adds "encoding=" mount option, usually not
                                  set.


BINDIR="</path/to/dir/containing/binaries>" Usually $WORKINGDIR/bin

#USERNAME=nobody
#GROUPNAME=nobody

RCMDPROG=<rsh|ssh>
RCPPROG=<rcp|scp>

SERVER="</path/to/pvfs2-server>" This specifies the path to the pvfs2-server
binary to be pushed out to all systems and executed, this is not the
path specifying where to run the pvfs2-server from, only where to get the pvfs2-server  
to use.  Same is true for PINGPROG.
PINGPROG="</path/to/pvfs2-ping/binary>"

COPYBINS=<1 = default, copy binaries out to temp area on each node,
          0 = do not copy, and run them from the specified path above>

TROVESYNC=<1 = default. sync data after metadata operations,
           0 = do not perform implicit sync. improved performance at the risk 
	   		of data loss>

3.) execute pav_start -c <configfile>.  You will see the pav system
setting up a PVFS volume for you.

4.) execute pav_info -c <configfile>.  This command will give you lots
of information about your newly created PVFS volume, including the
PVFS2TAB_FILE.

5.) run PVFS tests.  The volume is set up and ready to use!

6.) when finished, clean up using pav_stop -c configfile.  WARNING:
this command will kill all iod/mgr processes and clean up all data
stored in the associated data directories.

Thanks and good luck!  Any questions/comments should be sent to
pvfs2-developers@beowulf-underground.org
